{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779da4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/amlc2025-dataset/test_combined_embeddings.npy\n",
      "/kaggle/input/amlc2025-dataset/sample_test.csv\n",
      "/kaggle/input/amlc2025-dataset/sample_test_out.csv\n",
      "/kaggle/input/amlc2025-dataset/combined_embeddings.npy\n",
      "/kaggle/input/amlc2025-dataset/train.csv\n",
      "/kaggle/input/amlc2025-dataset/test.csv\n",
      "/kaggle/input/amlc2025-dataset/test_out_final.csv\n",
      "/kaggle/input/amlc2025-dataset/train_out.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d233d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "640e6396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa7a2baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7ddad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 14:26:32.313789: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-13 14:26:32.335063: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-13 14:26:32.341644: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Concatenate, Dropout, Bidirectional, LSTM, GRU\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73f51fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f402579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "870fb915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b4ad29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    N_SPLITS = 5\n",
    "    RANDOM_STATE = 42\n",
    "    DATA_PATH = '/kaggle/input/amlc2025-dataset/train.csv'\n",
    "    TEST_DATA_PATH = '/kaggle/input/amlc2025-dataset/test.csv'\n",
    "    \n",
    "    # Text Embedding Models\n",
    "    # Using a diverse pair: one for general semantics, one optimized for similarity\n",
    "    ST_MODELS = [\n",
    "        'sentence-transformers/all-MiniLM-L12-v2',\n",
    "        'BAAI/bge-base-en-v1.5',                     \n",
    "        'sentence-transformers/all-distilroberta-v1',\n",
    "        'sentence-transformers/paraphrase-mpnet-base-v2'\n",
    "    ]\n",
    "    BGE_MODEL_INDEX = 1 # The BGE model is the second in the list, at index 1\n",
    "    \n",
    "    # KNN Feature Generation\n",
    "    KNN_N_NEIGHBORS = 10\n",
    "    \n",
    "    # DenseNet (1D CNN) Config\n",
    "    # CNN_VOCAB_SIZE = 30000\n",
    "    # CNN_MAX_LEN = 60\n",
    "    # CNN_EMBEDDING_DIM = 128\n",
    "    DL_VOCAB_SIZE = 30000\n",
    "    DL_MAX_LEN = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "575042c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SMAPE metric function\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    ratio = np.where(denominator == 0, 0, numerator / denominator)\n",
    "    return np.mean(ratio) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c038bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extreme_engineered_features(df):\n",
    "    df_out = pd.DataFrame(index=df.index)\n",
    "    text_col = 'catalog_content'\n",
    "    \n",
    "    # Meta Features\n",
    "    df_out[f'{text_col}_length'] = df[text_col].str.len()\n",
    "    df_out[f'{text_col}_word_count'] = df[text_col].str.split().str.len()\n",
    "    df_out[f'{text_col}_capital_ratio'] = df[text_col].apply(lambda t: sum(1 for c in t if c.isupper()) / (len(t) + 1e-9))\n",
    "    \n",
    "    # Entity Extraction (Example lists - expand these!)\n",
    "    BRANDS = ['sony', 'samsung', 'nike', 'apple', 'kitchenaid']\n",
    "    MATERIALS = ['cotton', 'leather', 'silk', 'gold', 'nylon', 'polyester', 'silver']\n",
    "    \n",
    "    text_lower = df[text_col].str.lower()\n",
    "    df_out['has_brand'] = text_lower.apply(lambda t: any(brand in t for brand in BRANDS)).astype(int)\n",
    "    for material in MATERIALS:\n",
    "        df_out[f'has_{material}'] = text_lower.str.contains(material).astype(int)\n",
    "        \n",
    "    # Regex Features\n",
    "    df_out['extracted_inch'] = text_lower.str.extract(r'(\\d+\\.?\\d*)\\s*(inch|\\\"|in\\b)').iloc[:, 0].astype(float)\n",
    "    df_out['extracted_gb'] = text_lower.str.extract(r'(\\d+)\\s*gb').iloc[:, 0].astype(float)\n",
    "    \n",
    "    # Fill NaNs from regex with a neutral value like 0\n",
    "    return df_out.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0eae7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3: KNN feature generation function is ready.\n"
     ]
    }
   ],
   "source": [
    "# Replace your old KNN function with this improved one.\n",
    "# It's now located in SECTION 4 for clarity.\n",
    "\n",
    "# in SECTION 4.3\n",
    "def generate_knn_features(index_embeds, y_index, query_embeds, k):\n",
    "    \"\"\"\n",
    "    Generates KNN features.\n",
    "    'index_embeds' are used to build the search space.\n",
    "    'query_embeds' are the items we want to find neighbors for.\n",
    "    \"\"\"\n",
    "    d = index_embeds.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    \n",
    "    # Check if GPU is available\n",
    "    try:\n",
    "        res = faiss.StandardGpuResources()\n",
    "        gpu_index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "        gpu_index.add(index_embeds.astype(np.float32))\n",
    "        search_index = gpu_index\n",
    "    except AttributeError:\n",
    "        # Fallback to CPU if faiss-gpu is not installed or fails\n",
    "        print(\"   FAISS-GPU not found or failed, falling back to CPU.\")\n",
    "        index.add(index_embeds.astype(np.float32))\n",
    "        search_index = index\n",
    "\n",
    "    # If query is the same as index, we are doing self-search. K+1 and ignore first result.\n",
    "    is_self_search = np.array_equal(index_embeds, query_embeds)\n",
    "    if is_self_search:\n",
    "        _, I = search_index.search(query_embeds.astype(np.float32), k + 1)\n",
    "        I = I[:, 1:] # Exclude the first column which is the item itself\n",
    "    else:\n",
    "        _, I = search_index.search(query_embeds.astype(np.float32), k)\n",
    "    \n",
    "    neighbor_prices = y_index[I]\n",
    "    \n",
    "    knn_feats = np.zeros((len(query_embeds), 3))\n",
    "    knn_feats[:, 0] = np.mean(neighbor_prices, axis=1)\n",
    "    knn_feats[:, 1] = np.median(neighbor_prices, axis=1)\n",
    "    knn_feats[:, 2] = np.std(neighbor_prices, axis=1)\n",
    "    \n",
    "    return knn_feats\n",
    "\n",
    "print(\"3/3: KNN feature generation function is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95da98be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CFG.DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfdf57be",
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_engineered_features = create_extreme_engineered_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eca8944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, LSTM, GRU, Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e8cb96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model architectures are defined.\n"
     ]
    }
   ],
   "source": [
    "# Define the custom Attention layer\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
    "        super(Attention, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        et = tf.keras.backend.squeeze(tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b), axis=-1)\n",
    "        at = tf.keras.backend.softmax(et)\n",
    "        at = tf.keras.backend.expand_dims(at, axis=-1)\n",
    "        output = x * at\n",
    "        return tf.keras.backend.sum(output, axis=1)\n",
    "\n",
    "# Model 3: LSTM with Attention\n",
    "def create_lstm_attention_model(vocab_size, max_len, embedding_dim):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    embedding = Embedding(vocab_size, embedding_dim)(inputs)\n",
    "    bilstm = Bidirectional(LSTM(128, return_sequences=True))(embedding)\n",
    "    attention = Attention()(bilstm)\n",
    "    dense = Dense(64, activation='relu')(attention)\n",
    "    outputs = Dense(1)(dense)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Model 4: 1D CNN with Multiple Kernels\n",
    "def create_multikernel_cnn_model(vocab_size, max_len, embedding_dim):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    embedding = Embedding(vocab_size, embedding_dim)(inputs)\n",
    "    \n",
    "    conv_layers = []\n",
    "    for kernel_size in [2, 3, 5]:\n",
    "        conv = Conv1D(filters=64, kernel_size=kernel_size, activation='relu')(embedding)\n",
    "        pool = GlobalMaxPooling1D()(conv)\n",
    "        conv_layers.append(pool)\n",
    "    \n",
    "    concatenated = Concatenate()(conv_layers)\n",
    "    dense = Dense(128, activation='relu')(concatenated)\n",
    "    dropout = Dropout(0.5)(dense)\n",
    "    outputs = Dense(1)(dropout)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Model 5: Multi-Input Neural Network\n",
    "def create_multi_input_model(vocab_size, max_len, embedding_dim, num_tabular_feats):\n",
    "    # Text input branch\n",
    "    text_input = Input(shape=(max_len,), name='text_input')\n",
    "    embedding = Embedding(vocab_size, embedding_dim)(text_input)\n",
    "    gru = GRU(64)(embedding)\n",
    "    text_branch = Dense(32, activation='relu')(gru)\n",
    "    \n",
    "    # Tabular input branch\n",
    "    tabular_input = Input(shape=(num_tabular_feats,), name='tabular_input')\n",
    "    tabular_branch = Dense(32, activation='relu')(tabular_input)\n",
    "    \n",
    "    # Merge branches\n",
    "    concatenated = Concatenate()([text_branch, tabular_branch])\n",
    "    dense = Dense(64, activation='relu')(concatenated)\n",
    "    outputs = Dense(1)(dense)\n",
    "    \n",
    "    return Model(inputs=[text_input, tabular_input], outputs=outputs)\n",
    "\n",
    "print(\"New model architectures are defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a16e45c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_embeddings = np.load('/kaggle/input/amlc2025-dataset/' + 'combined_embeddings.npy').astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "067126f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_xgb = np.zeros(len(df))\n",
    "oof_lgbm = np.zeros(len(df))\n",
    "oof_lstm_att = np.zeros(len(df))\n",
    "oof_cnn_multi = np.zeros(len(df))\n",
    "oof_nn_multi_input = np.zeros(len(df))\n",
    "X_text = df['catalog_content']\n",
    "y = df['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f41cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=CFG.N_SPLITS, shuffle=True, random_state=CFG.RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40c00b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating embedding dimensions for KNN slicing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f831f40ad5c4888990e06a2aada7f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a5018dfe3044fbb201f55580bea71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7052f6f7a7384dabb8c73550514b7163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdb9285bb3a45b2a721b8f2719c4566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd086f69103a4d48a0935c3f5249a516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b596928ff94a4b859cdefc6487c75c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4daccd08519a48c6bcd4d5f8acb81dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/352 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd7ecfa9c034dc49c24abf8c46a1d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "479aa809afda4453b42b9b8d3c87ad18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5810ee09e93045cbb8d5ebc09dbd7a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8eccc9792b460eae5be30ff45d5b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f654dcd6acfc424d81528c7bef7c0e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f9cdbb0f884f4883c9716291853bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf90ea65f0f14ec097dbce82121df36c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed8a1b4c47c4acbaaaf057f61ac4fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53322b8727cf41c78b3f45db549b07a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287ee80d7cd84f4283cb33be6bbb9194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746aabfb4ecd47a796ac60d12589b340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fa7879270d4b4daed7e4321fbbb624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76109251a5674ce0abaece83a0d8c13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da63ebc57aef4034bf57c5d64707c9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0395b05d8d44fc19a050d64175477f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8cd6868582640ce90582892f59e8023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04681f47a3cf4db4a5cdcce6d5a937f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1999ce88e7ea4bb3b822e9c180aa4e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc87565194284efc8dfa4ad924bda740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20554612b44f4ae891c13f9252b791a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8f35fac56c48eea03e0b6b4f186d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb5aa06b01a4c56aa71d85a57194e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d681904c98124f8c89a7d478f1d3c641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77bff13b3a2b435492faf332150f9b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c43d78763f84707bb9da3e47e261055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01f405357714ef6b45069dac4e7b57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626cc8cf1458423fa734d3c36ff9e9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4b81745a3144789bae81a0582defee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c86f47971064a3aa2ea177a8c8621cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb002a6df81492f8332b6c1879ed5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5952c03447e460888300c20d8e94304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e8380b03b24a7d8f67f7c5195176b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/594 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a78dc73c8ce41ed8aa05e896d2dc191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af18642698c345e1810212b98009acef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e34f7456d443b39600341a8a165f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e8b4ffddec4f72b24de96cf81828d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d73e50214c4bb4a9bef07adfde95ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8533b74be19a4b26bf63757bf47da617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGE embeddings will be sliced from column 384 to 1152.\n"
     ]
    }
   ],
   "source": [
    "# Pre-calculate the dimensions of each sentence transformer model to find the BGE slice\n",
    "print(\"Calculating embedding dimensions for KNN slicing...\")\n",
    "model_dims = [SentenceTransformer(m).get_sentence_embedding_dimension() for m in CFG.ST_MODELS]\n",
    "bge_start_index = sum(model_dims[:CFG.BGE_MODEL_INDEX])\n",
    "bge_end_index = bge_start_index + model_dims[CFG.BGE_MODEL_INDEX]\n",
    "print(f\"BGE embeddings will be sliced from column {bge_start_index} to {bge_end_index}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96bf05cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FOLD 1 / 5 =====\n",
      "1/5: Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fold 1 XGB SMAPE: 63.3805%\n",
      "2/5: Training LightGBM...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 687204\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2703\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 2694 dense feature groups (154.27 MB) transferred to GPU in 0.146713 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 23.598634\n",
      "   Fold 1 LGBM SMAPE: 61.3546%\n",
      "   Tokenizing text for DL models...\n",
      "3/5: Training LSTM w/ Attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760365928.275846     235 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1760365928.276543     235 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 206ms/step\n",
      "   Fold 1 LSTM w/ Attention SMAPE: 61.3010%\n",
      "4/5: Training Multi-Kernel CNN...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "   Fold 1 Multi-Kernel CNN SMAPE: 61.5148%\n",
      "5/5: Training Multi-Input NN...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step\n",
      "   Fold 1 Multi-Input NN SMAPE: nan%\n",
      "--- Fold Complete. Cleaning memory. ---\n",
      "\n",
      "===== FOLD 2 / 5 =====\n",
      "1/5: Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fold 2 XGB SMAPE: 62.2772%\n",
      "2/5: Training LightGBM...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 687204\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2703\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 2694 dense feature groups (154.27 MB) transferred to GPU in 0.149166 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 23.620979\n",
      "   Fold 2 LGBM SMAPE: 60.5203%\n",
      "   Tokenizing text for DL models...\n",
      "3/5: Training LSTM w/ Attention...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 207ms/step\n",
      "   Fold 2 LSTM w/ Attention SMAPE: 67.8961%\n",
      "4/5: Training Multi-Kernel CNN...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "   Fold 2 Multi-Kernel CNN SMAPE: 61.6807%\n",
      "5/5: Training Multi-Input NN...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step\n",
      "   Fold 2 Multi-Input NN SMAPE: nan%\n",
      "--- Fold Complete. Cleaning memory. ---\n",
      "\n",
      "===== FOLD 3 / 5 =====\n",
      "1/5: Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fold 3 XGB SMAPE: 62.6807%\n",
      "2/5: Training LightGBM...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 687203\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2703\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 2694 dense feature groups (154.27 MB) transferred to GPU in 0.148882 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 23.709702\n",
      "   Fold 3 LGBM SMAPE: 60.6524%\n",
      "   Tokenizing text for DL models...\n",
      "3/5: Training LSTM w/ Attention...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 189ms/step\n",
      "   Fold 3 LSTM w/ Attention SMAPE: 62.2922%\n",
      "4/5: Training Multi-Kernel CNN...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "   Fold 3 Multi-Kernel CNN SMAPE: 61.2121%\n",
      "5/5: Training Multi-Input NN...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step\n",
      "   Fold 3 Multi-Input NN SMAPE: 62.6097%\n",
      "--- Fold Complete. Cleaning memory. ---\n",
      "\n",
      "===== FOLD 4 / 5 =====\n",
      "1/5: Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fold 4 XGB SMAPE: 61.9190%\n",
      "2/5: Training LightGBM...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 687204\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2703\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 2694 dense feature groups (154.27 MB) transferred to GPU in 0.148355 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 23.677128\n",
      "   Fold 4 LGBM SMAPE: 62.5231%\n",
      "   Tokenizing text for DL models...\n",
      "3/5: Training LSTM w/ Attention...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 188ms/step\n",
      "   Fold 4 LSTM w/ Attention SMAPE: 61.3268%\n",
      "4/5: Training Multi-Kernel CNN...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
      "   Fold 4 Multi-Kernel CNN SMAPE: 60.1618%\n",
      "5/5: Training Multi-Input NN...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step\n",
      "   Fold 4 Multi-Input NN SMAPE: nan%\n",
      "--- Fold Complete. Cleaning memory. ---\n",
      "\n",
      "===== FOLD 5 / 5 =====\n",
      "1/5: Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fold 5 XGB SMAPE: 73.7448%\n",
      "2/5: Training LightGBM...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 687202\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2703\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 2694 dense feature groups (154.27 MB) transferred to GPU in 0.147139 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 23.631827\n",
      "   Fold 5 LGBM SMAPE: 60.4405%\n",
      "   Tokenizing text for DL models...\n",
      "3/5: Training LSTM w/ Attention...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 189ms/step\n",
      "   Fold 5 LSTM w/ Attention SMAPE: 64.2732%\n",
      "4/5: Training Multi-Kernel CNN...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "   Fold 5 Multi-Kernel CNN SMAPE: 60.8016%\n",
      "5/5: Training Multi-Input NN...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step\n",
      "   Fold 5 Multi-Input NN SMAPE: nan%\n",
      "--- Fold Complete. Cleaning memory. ---\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "    print(f\"\\n===== FOLD {fold+1} / {CFG.N_SPLITS} =====\")\n",
    "    \n",
    "    # --- Split All Data for this Fold ---\n",
    "    X_train_text, X_val_text = df['catalog_content'].iloc[train_idx], df['catalog_content'].iloc[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    X_train_eng, X_val_eng = extreme_engineered_features.iloc[train_idx].values.astype(np.float32), extreme_engineered_features.iloc[val_idx].values.astype(np.float32)\n",
    "    X_train_embed, X_val_embed = combined_embeddings[train_idx], combined_embeddings[val_idx]\n",
    "\n",
    "    # --- Model 1: XGBoost (Keyword Expert) ---\n",
    "    print(\"1/5: Training XGBoost...\")\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1, 2), max_features=25000)\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "    X_val_tfidf = tfidf.transform(X_val_text)\n",
    "    \n",
    "    X_train_xgb = scipy.sparse.hstack((X_train_tfidf, X_train_eng))\n",
    "    X_val_xgb = scipy.sparse.hstack((X_val_tfidf, X_val_eng))\n",
    "\n",
    "    # Using default but robust parameters\n",
    "    xgb_model = xgb.XGBRegressor(tree_method='hist', device='cuda')\n",
    "    xgb_model.fit(X_train_xgb, y_train, eval_set=[(X_val_xgb, y_val)], early_stopping_rounds=50, verbose=False)\n",
    "    oof_xgb[val_idx] = xgb_model.predict(X_val_xgb)\n",
    "    print(f\"   Fold {fold+1} XGB SMAPE: {smape(y_val, oof_xgb[val_idx]):.4f}%\")\n",
    "\n",
    "    # --- Model 2: LightGBM (Semantic & Market Expert) ---\n",
    "    print(\"2/5: Training LightGBM...\")\n",
    "    bge_train_embeds = X_train_embed[:, bge_start_index:bge_end_index]\n",
    "    bge_val_embeds = X_val_embed[:, bge_start_index:bge_end_index]\n",
    "    \n",
    "    knn_feats_train = generate_knn_features(bge_train_embeds, y_train, bge_train_embeds, CFG.KNN_N_NEIGHBORS)\n",
    "    knn_feats_val = generate_knn_features(bge_train_embeds, y_train, bge_val_embeds, CFG.KNN_N_NEIGHBORS)\n",
    "\n",
    "    X_train_lgbm = np.hstack([X_train_embed, X_train_eng, knn_feats_train])\n",
    "    X_val_lgbm = np.hstack([X_val_embed, X_val_eng, knn_feats_val])\n",
    "    \n",
    "    # Using default but robust parameters\n",
    "    lgbm_model = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=40, subsample=0.8, colsample_bytree=0.8, device='gpu', random_state=CFG.RANDOM_STATE)\n",
    "    lgbm_model.fit(X_train_lgbm, y_train, eval_set=[(X_val_lgbm, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    oof_lgbm[val_idx] = lgbm_model.predict(X_val_lgbm)\n",
    "    print(f\"   Fold {fold+1} LGBM SMAPE: {smape(y_val, oof_lgbm[val_idx]):.4f}%\")\n",
    "\n",
    "    # --- Tokenize text for all DL models (do this once per fold) ---\n",
    "    print(\"   Tokenizing text for DL models...\")\n",
    "    tokenizer = Tokenizer(num_words=CFG.DL_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train_text)\n",
    "    X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train_text), maxlen=CFG.DL_MAX_LEN)\n",
    "    X_val_seq = pad_sequences(tokenizer.texts_to_sequences(X_val_text), maxlen=CFG.DL_MAX_LEN)\n",
    "\n",
    "    # Common settings for DL model training\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=0)\n",
    "    dl_batch_size = 256\n",
    "    dl_epochs = 30\n",
    "\n",
    "    # --- Model 3: LSTM with Attention ---\n",
    "    print(\"3/5: Training LSTM w/ Attention...\")\n",
    "    model_lstm = create_lstm_attention_model(CFG.DL_VOCAB_SIZE, CFG.DL_MAX_LEN, 128)\n",
    "    model_lstm.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    model_lstm.fit(X_train_seq, y_train, validation_data=(X_val_seq, y_val),\n",
    "                   epochs=dl_epochs, batch_size=dl_batch_size, callbacks=[es], verbose=0)\n",
    "    oof_lstm_att[val_idx] = model_lstm.predict(X_val_seq, batch_size=dl_batch_size*2).squeeze()\n",
    "    print(f\"   Fold {fold+1} LSTM w/ Attention SMAPE: {smape(y_val, oof_lstm_att[val_idx]):.4f}%\")\n",
    "    del model_lstm; gc.collect()\n",
    "\n",
    "    # --- Model 4: CNN with Multi-Kernel ---\n",
    "    print(\"4/5: Training Multi-Kernel CNN...\")\n",
    "    model_cnn = create_multikernel_cnn_model(CFG.DL_VOCAB_SIZE, CFG.DL_MAX_LEN, 128)\n",
    "    model_cnn.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    model_cnn.fit(X_train_seq, y_train, validation_data=(X_val_seq, y_val),\n",
    "                  epochs=dl_epochs, batch_size=dl_batch_size, callbacks=[es], verbose=0)\n",
    "    oof_cnn_multi[val_idx] = model_cnn.predict(X_val_seq, batch_size=dl_batch_size*2).squeeze()\n",
    "    print(f\"   Fold {fold+1} Multi-Kernel CNN SMAPE: {smape(y_val, oof_cnn_multi[val_idx]):.4f}%\")\n",
    "    del model_cnn; gc.collect()\n",
    "    \n",
    "    # --- Model 5: Multi-Input NN ---\n",
    "    print(\"5/5: Training Multi-Input NN...\")\n",
    "    # Assemble the specific tabular features for this model\n",
    "    tabular_train_feats = np.hstack([X_train_eng, knn_feats_train])\n",
    "    tabular_val_feats = np.hstack([X_val_eng, knn_feats_val])\n",
    "    \n",
    "    model_multi_input = create_multi_input_model(CFG.DL_VOCAB_SIZE, CFG.DL_MAX_LEN, 128, tabular_train_feats.shape[1])\n",
    "    model_multi_input.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    model_multi_input.fit([X_train_seq, tabular_train_feats], y_train,\n",
    "                          validation_data=([X_val_seq, tabular_val_feats], y_val),\n",
    "                          epochs=dl_epochs, batch_size=dl_batch_size, callbacks=[es], verbose=0)\n",
    "    \n",
    "    preds = model_multi_input.predict([X_val_seq, tabular_val_feats], batch_size=dl_batch_size*2)\n",
    "    oof_nn_multi_input[val_idx] = preds.squeeze()\n",
    "    print(f\"   Fold {fold+1} Multi-Input NN SMAPE: {smape(y_val, oof_nn_multi_input[val_idx]):.4f}%\")\n",
    "    del model_multi_input; gc.collect()\n",
    "\n",
    "    # --- Clean up at the end of the fold ---\n",
    "    print(\"--- Fold Complete. Cleaning memory. ---\")\n",
    "    del X_train_text, X_val_text, y_train, y_val, X_train_eng, X_val_eng, X_train_embed, X_val_embed\n",
    "    del X_train_tfidf, X_val_tfidf, X_train_xgb, X_val_xgb, xgb_model\n",
    "    del bge_train_embeds, bge_val_embeds, knn_feats_train, knn_feats_val, X_train_lgbm, X_val_lgbm, lgbm_model\n",
    "    del tokenizer, X_train_seq, X_val_seq, tabular_train_feats, tabular_val_feats\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb552e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Final Submission File ---\n",
      "External predictions merged successfully. Shape: (75000,)\n",
      "Final meta-features shape: (75000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Generating Final Submission File ---\")\n",
    "\n",
    "EXTERNAL_TRAIN_PREDS_PATH = '/kaggle/input/amlc2025-dataset/train_out.csv'\n",
    "EXTERNAL_PRED_COLUMN_NAME = 'price' # The name of the prediction column in your CSV\n",
    "ID_COLUMN = 'sample_id'\n",
    "\n",
    "df_train_out = pd.read_csv(EXTERNAL_TRAIN_PREDS_PATH)\n",
    "\n",
    "merged_train_preds = pd.merge(df[[ID_COLUMN]], df_train_out[[ID_COLUMN, EXTERNAL_PRED_COLUMN_NAME]], on=ID_COLUMN, how='left')\n",
    "merged_train_preds[EXTERNAL_PRED_COLUMN_NAME] = merged_train_preds[EXTERNAL_PRED_COLUMN_NAME].fillna(0)\n",
    "\n",
    "# Extract the predictions as a NumPy array in the correct order\n",
    "external_oof_preds = merged_train_preds[EXTERNAL_PRED_COLUMN_NAME].values\n",
    "\n",
    "print(f\"External predictions merged successfully. Shape: {external_oof_preds.shape}\")\n",
    "\n",
    "X_meta = np.column_stack((\n",
    "    oof_xgb, \n",
    "    oof_lgbm, \n",
    "    oof_lstm_att, \n",
    "    oof_cnn_multi, \n",
    "    oof_nn_multi_input,\n",
    "    external_oof_preds  # Adding the 6th feature\n",
    "))\n",
    "print(f\"Final meta-features shape: {X_meta.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a149c20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Meta-features shape: {X_meta.shape}\")\n",
    "\n",
    "# # --- 2. Train the Ridge Regressor ---\n",
    "# # A simple, robust Ridge model is an excellent choice for a meta-model.\n",
    "# # It learns the optimal linear combination of the base model predictions.\n",
    "# meta_model = Ridge(alpha=1.0)\n",
    "# meta_model.fit(X_meta, y)\n",
    "\n",
    "# print(\"Meta-model trained successfully.\")\n",
    "\n",
    "# # --- 3. Evaluate the Full Ensemble ---\n",
    "# # Predict on the OOFs to get the final, blended cross-validation score.\n",
    "# # This is the most reliable estimate of your leaderboard score.\n",
    "# final_oof_preds = meta_model.predict(X_meta)\n",
    "# final_oof_smape = smape(y, final_oof_preds)\n",
    "\n",
    "# print(f\"\\nOverall OOF SMAPE of the full 5-model ensemble: {final_oof_smape:.4f}%\")\n",
    "\n",
    "# # Print the weights to see which models were most important\n",
    "# model_names = ['XGB', 'LGBM', 'LSTM_Att', 'CNN_Multi', 'NN_Multi_Input', 'image_model']\n",
    "# print(\"Meta-Model Weights:\")\n",
    "# for name, coef in zip(model_names, meta_model.coef_):\n",
    "#     print(f\"   {name}: {coef:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcb57d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking for NaNs in OOF predictions ---\n",
      "WARNING: Found 60000 NaN values in OOF predictions for model: NN_Multi_Input\n",
      "Imputing NaN values with the median of each column...\n",
      "NaNs imputed successfully.\n",
      "Meta-model trained successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Train the Ridge Regressor ---\n",
    "# --- NEW: DIAGNOSTIC AND FIX BLOCK ---\n",
    "print(\"\\n--- Checking for NaNs in OOF predictions ---\")\n",
    "\n",
    "# Diagnostic: Find out which model is causing the problem\n",
    "model_names = ['XGB', 'LGBM', 'LSTM_Att', 'CNN_Multi', 'NN_Multi_Input', 'External']\n",
    "for i, name in enumerate(model_names):\n",
    "    nan_count = np.isnan(X_meta[:, i]).sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"WARNING: Found {nan_count} NaN values in OOF predictions for model: {name}\")\n",
    "\n",
    "# Immediate Fix: Impute NaNs with the column median\n",
    "if np.isnan(X_meta).any():\n",
    "    print(\"Imputing NaN values with the median of each column...\")\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_meta = imputer.fit_transform(X_meta)\n",
    "    print(\"NaNs imputed successfully.\")\n",
    "\n",
    "# --- 2. Train the Ridge Regressor (this will now work) ---\n",
    "meta_model = Ridge(alpha=1.0)\n",
    "meta_model.fit(X_meta, y)\n",
    "print(\"Meta-model trained successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "010da508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall OOF SMAPE of the full 6-model ensemble: 47.3280%\n",
      "Meta-Model Weights:\n",
      "   XGB: 0.3567\n",
      "   LGBM: 0.2251\n",
      "   LSTM_Att: 0.0009\n",
      "   CNN_Multi: 0.1921\n",
      "   NN_Multi_Input: -0.0000\n",
      "   External_Model: 0.4778\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Evaluate the Full Ensemble ---\n",
    "final_oof_preds = meta_model.predict(X_meta)\n",
    "final_oof_smape = smape(y, final_oof_preds)\n",
    "print(f\"\\nOverall OOF SMAPE of the full 6-model ensemble: {final_oof_smape:.4f}%\")\n",
    "\n",
    "# Print the weights to see the importance of the external model\n",
    "model_names = ['XGB', 'LGBM', 'LSTM_Att', 'CNN_Multi', 'NN_Multi_Input', 'External_Model']\n",
    "print(\"Meta-Model Weights:\")\n",
    "for name, coef in zip(model_names, meta_model.coef_):\n",
    "    print(f\"   {name}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1c2b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(CFG.TEST_DATA_PATH)\n",
    "test_extreme_engineered_features = create_extreme_engineered_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d98c7b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_combined_embeddings = np.load('/kaggle/input/amlc2025-dataset/' + 'test_combined_embeddings.npy').astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a23653fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_engineered = create_extreme_engineered_features(df)\n",
    "test_engineered = create_extreme_engineered_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57f87b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Features (CRITICAL: Fit on full train, then transform both)\n",
    "final_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=25000)\n",
    "full_train_tfidf = final_tfidf_vectorizer.fit_transform(df['catalog_content'])\n",
    "X_test_tfidf = final_tfidf_vectorizer.transform(df_test['catalog_content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e87d0427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Features (Index on full train, query for both train and test)\n",
    "full_train_bge_embeds = combined_embeddings[:, bge_start_index:bge_end_index]\n",
    "test_bge_embeds = test_combined_embeddings[:, bge_start_index:bge_end_index]\n",
    "final_train_knn_feats = generate_knn_features(full_train_bge_embeds, y, full_train_bge_embeds, CFG.KNN_N_NEIGHBORS).astype(np.float32)\n",
    "test_knn_feats = generate_knn_features(full_train_bge_embeds, y, test_bge_embeds, CFG.KNN_N_NEIGHBORS).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d45f15bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3/4: Re-training base models on full data and generating test predictions...\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer for DL Models (CRITICAL: Fit on full train, then transform both)\n",
    "final_tokenizer = Tokenizer(num_words=CFG.DL_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "final_tokenizer.fit_on_texts(df['catalog_content'])\n",
    "full_train_seq = pad_sequences(final_tokenizer.texts_to_sequences(df['catalog_content']), maxlen=CFG.DL_MAX_LEN)\n",
    "X_test_seq = pad_sequences(final_tokenizer.texts_to_sequences(df_test['catalog_content']), maxlen=CFG.DL_MAX_LEN)\n",
    "\n",
    "\n",
    "# --- 3. Re-train Base Models on 100% of the Training Data and Predict on Test ---\n",
    "print(\"\\n3/4: Re-training base models on full data and generating test predictions...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ab47fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training final XGBoost model...\n",
      "   ...done.\n"
     ]
    }
   ],
   "source": [
    "# --- Final Model 1: XGBoost ---\n",
    "print(\"   Training final XGBoost model...\")\n",
    "full_train_xgb = scipy.sparse.hstack((full_train_tfidf, full_train_engineered.values))\n",
    "X_test_xgb = scipy.sparse.hstack((X_test_tfidf, test_engineered.values))\n",
    "# Use the tuned hyperparameters from your CFG class\n",
    "# final_xgb_model = xgb.XGBRegressor(**CFG.XGB_PARAMS)\n",
    "final_xgb_model = xgb.XGBRegressor(tree_method='hist', device='cuda')\n",
    "# We don't use early stopping here, we want the model to train for all n_estimators\n",
    "final_xgb_model.fit(full_train_xgb, y)\n",
    "test_preds_xgb = final_xgb_model.predict(X_test_xgb)\n",
    "print(\"   ...done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4255700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training final LightGBM model...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 687208\n",
      "[LightGBM] [Info] Number of data points in the train set: 75000, number of used features: 2703\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 2694 dense feature groups (192.83 MB) transferred to GPU in 0.185837 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 23.647654\n",
      "   ...done.\n"
     ]
    }
   ],
   "source": [
    "# --- Final Model 2: LightGBM ---\n",
    "print(\"   Training final LightGBM model...\")\n",
    "full_train_lgbm = np.hstack([combined_embeddings, full_train_engineered.values, final_train_knn_feats])\n",
    "X_test_lgbm = np.hstack([test_combined_embeddings, test_engineered.values, test_knn_feats])\n",
    "# final_lgbm_model = lgb.LGBMRegressor(**CFG.LGBM_PARAMS)\n",
    "final_lgbm_model = lgb.LGBMRegressor(device='gpu')\n",
    "final_lgbm_model.fit(full_train_lgbm, y)\n",
    "test_preds_lgbm = final_lgbm_model.predict(X_test_lgbm)\n",
    "print(\"   ...done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66b26fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training final LSTM w/ Attention model...\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 180ms/step\n",
      "   ...done.\n"
     ]
    }
   ],
   "source": [
    "# # --- Final Model 3: LSTM w/ Attention ---\n",
    "# print(\"   Training final LSTM w/ Attention model...\")\n",
    "# # lstm_params = CFG.LSTM_ATT_PARAMS.copy()\n",
    "# # lr = lstm_params.pop('learning_rate')\n",
    "# lr = 0.001\n",
    "# final_lstm_model = create_lstm_attention_model(CFG.DL_VOCAB_SIZE, CFG.DL_MAX_LEN)\n",
    "# final_lstm_model.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
    "# # Train on full data for a fixed number of epochs (e.g., the average from your OOF runs)\n",
    "# final_lstm_model.fit(full_train_seq, y, epochs=18, batch_size=256, verbose=0)\n",
    "# test_preds_lstm_att = final_lstm_model.predict(X_test_seq, batch_size=512).squeeze()\n",
    "# print(\"   ...done.\")\n",
    "\n",
    "# --- Final Model 3: LSTM w/ Attention ---\n",
    "print(\"   Training final LSTM w/ Attention model...\")\n",
    "\n",
    "# Set the learning rate (as you were doing)\n",
    "lr = 0.001\n",
    "\n",
    "# --- THIS IS THE CORRECTED LINE ---\n",
    "# We now provide the missing third argument for embedding_dim. 128 is a robust choice.\n",
    "final_lstm_model = create_lstm_attention_model(CFG.DL_VOCAB_SIZE, CFG.DL_MAX_LEN, 128) \n",
    "\n",
    "# The rest of the code remains the same\n",
    "final_lstm_model.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
    "# Train on full data for a fixed number of epochs\n",
    "final_lstm_model.fit(full_train_seq, y, epochs=18, batch_size=256, verbose=0)\n",
    "test_preds_lstm_att = final_lstm_model.predict(X_test_seq, batch_size=512).squeeze()\n",
    "print(\"   ...done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce4af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "323c89b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training final Multi-Kernel CNN model...\n",
      "Epoch 1/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 85ms/step - loss: 1088.2666\n",
      "Epoch 2/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 84ms/step - loss: 714.2386\n",
      "Epoch 3/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 84ms/step - loss: 685.3861\n",
      "Epoch 4/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 85ms/step - loss: 560.0074\n",
      "Epoch 5/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 84ms/step - loss: 497.7874\n",
      "Epoch 6/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 85ms/step - loss: 372.4983\n",
      "Epoch 7/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 85ms/step - loss: 351.5409\n",
      "Epoch 8/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 86ms/step - loss: 278.7570\n",
      "Epoch 9/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 85ms/step - loss: 275.6773\n",
      "Epoch 10/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 86ms/step - loss: 266.8417\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 31ms/step\n",
      "   ...done.\n"
     ]
    }
   ],
   "source": [
    "# --- Final Model 4: Multi-Kernel CNN ---\n",
    "print(\"   Training final Multi-Kernel CNN model...\")\n",
    "# cnn_params = CFG.CNN_MULTI_PARAMS.copy()\n",
    "# lr = cnn_params.pop('learning_rate')\n",
    "lr = 0.001\n",
    "final_cnn_model = create_multikernel_cnn_model(CFG.DL_VOCAB_SIZE, CFG.DL_MAX_LEN, 128)\n",
    "final_cnn_model.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
    "final_cnn_model.fit(full_train_seq, y, epochs=10, batch_size=256, verbose=1)\n",
    "test_preds_cnn_multi = final_cnn_model.predict(X_test_seq, batch_size=512).squeeze()\n",
    "print(\"   ...done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6efd141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training final Multi-Input NN model...\n",
      "Epoch 1/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 99ms/step - loss: nan\n",
      "Epoch 2/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 100ms/step - loss: nan\n",
      "Epoch 3/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 101ms/step - loss: nan\n",
      "Epoch 4/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 102ms/step - loss: nan\n",
      "Epoch 5/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - loss: nan\n",
      "Epoch 6/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 99ms/step - loss: nan\n",
      "Epoch 7/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 98ms/step - loss: nan\n",
      "Epoch 8/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 98ms/step - loss: nan\n",
      "Epoch 9/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 98ms/step - loss: nan\n",
      "Epoch 10/10\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 98ms/step - loss: nan\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step\n",
      "   ...done.\n"
     ]
    }
   ],
   "source": [
    "# # --- Final Model 5: Multi-Input NN ---\n",
    "# print(\"   Training final Multi-Input NN model...\")\n",
    "# full_tabular_feats = np.hstack([full_train_engineered.values, final_train_knn_feats])\n",
    "# test_tabular_feats = np.hstack([test_engineered.values, test_knn_feats])\n",
    "# # multi_input_params = CFG.NN_MULTI_INPUT_PARAMS.copy()\n",
    "# # lr = multi_input_params.pop('learning_rate')\n",
    "# lr = 0.001\n",
    "# final_multi_input_model = create_multi_input_model(CFG.DL_VOCAB_SIZE, CFG.DL_MAX_LEN, full_tabular_feats.shape[1])\n",
    "# final_multi_input_model.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
    "# final_multi_input_model.fit([full_train_seq, full_tabular_feats], y, epochs=10, batch_size=256, verbose=1)\n",
    "# test_preds_nn_multi_input = final_multi_input_model.predict([X_test_seq, test_tabular_feats], batch_size=512).squeeze()\n",
    "# print(\"   ...done.\")\n",
    "\n",
    "# --- Final Model 5: Multi-Input NN ---\n",
    "print(\"   Training final Multi-Input NN model...\")\n",
    "\n",
    "# First, assemble the tabular features for both train and test sets\n",
    "full_tabular_feats = np.hstack([full_train_engineered.values, final_train_knn_feats])\n",
    "test_tabular_feats = np.hstack([test_engineered.values, test_knn_feats])\n",
    "\n",
    "# Set the learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# --- THIS IS THE CORRECTED LINE ---\n",
    "# We now provide all four required arguments in the correct order:\n",
    "# 1. vocab_size\n",
    "# 2. max_len\n",
    "# 3. embedding_dim (the missing one)\n",
    "# 4. num_tabular_feats\n",
    "final_multi_input_model = create_multi_input_model(\n",
    "    CFG.DL_VOCAB_SIZE, \n",
    "    CFG.DL_MAX_LEN, \n",
    "    128,  # The missing embedding_dim\n",
    "    full_tabular_feats.shape[1]\n",
    ")\n",
    "\n",
    "# The rest of the code remains the same\n",
    "final_multi_input_model.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
    "final_multi_input_model.fit([full_train_seq, full_tabular_feats], y, epochs=10, batch_size=256, verbose=1)\n",
    "test_preds_nn_multi_input = final_multi_input_model.predict([X_test_seq, test_tabular_feats], batch_size=512).squeeze()\n",
    "print(\"   ...done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61bfc3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4/4: All test predictions have been generated successfully.\n",
      "Shape of test_preds_xgb: (75000,)\n",
      "Shape of test_preds_lgbm: (75000,)\n",
      "Shape of test_preds_lstm_att: (75000,)\n",
      "Shape of test_preds_cnn_multi: (75000,)\n",
      "Shape of test_preds_nn_multi_input: (75000,)\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Final Sanity Check ---\n",
    "print(\"\\n4/4: All test predictions have been generated successfully.\")\n",
    "print(f\"Shape of test_preds_xgb: {test_preds_xgb.shape}\")\n",
    "print(f\"Shape of test_preds_lgbm: {test_preds_lgbm.shape}\")\n",
    "print(f\"Shape of test_preds_lstm_att: {test_preds_lstm_att.shape}\")\n",
    "print(f\"Shape of test_preds_cnn_multi: {test_preds_cnn_multi.shape}\")\n",
    "print(f\"Shape of test_preds_nn_multi_input: {test_preds_nn_multi_input.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2cc4cbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Final Submission File with Robust NaN Handling ---\n",
      "Loading external test predictions from: /kaggle/input/amlc2025-dataset/test_out_final.csv\n",
      "External test predictions merged successfully. Shape: (75000,)\n",
      "\n",
      "--- Checking for NaNs in FINAL test predictions ---\n",
      "WARNING: Found 75000 NaN values in TEST predictions for model: NN_Multi_Input\n",
      "Replacing NaN values with 0...\n",
      "NaNs replaced successfully.\n",
      "\n",
      "Making final predictions with the meta-model...\n",
      "\n",
      "Creating submission.csv...\n",
      "\n",
      "Submission file created successfully!\n",
      "Top 5 rows of submission.csv:\n",
      "       id      price\n",
      "0  100179  17.905004\n",
      "1  245611  16.870453\n",
      "2  146263  20.393772\n",
      "3   95658   9.268154\n",
      "4   36806  56.891477\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 8: FINAL SUBMISSION (WITH ROBUST NaN HANDLING)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Generating Final Submission File with Robust NaN Handling ---\")\n",
    "\n",
    "# --- (This assumes you have already run the code to generate all test_preds_... variables) ---\n",
    "# --- (And that you have the trained `meta_model` in memory) ---\n",
    "\n",
    "# --- 1. Load and Merge External Test Predictions ---\n",
    "EXTERNAL_TEST_PREDS_PATH = '/kaggle/input/amlc2025-dataset/test_out_final.csv'\n",
    "ID_COLUMN = 'sample_id'\n",
    "EXTERNAL_PRED_COLUMN_NAME = 'price'\n",
    "\n",
    "print(f\"Loading external test predictions from: {EXTERNAL_TEST_PREDS_PATH}\")\n",
    "df_test_out = pd.read_csv(EXTERNAL_TEST_PREDS_PATH)\n",
    "merged_test_preds = pd.merge(df_test[[ID_COLUMN]], df_test_out[[ID_COLUMN, EXTERNAL_PRED_COLUMN_NAME]], on=ID_COLUMN, how='left')\n",
    "# Your .fillna(0) here is perfect for handling IDs missing from the external file\n",
    "merged_test_preds[EXTERNAL_PRED_COLUMN_NAME] = merged_test_preds[EXTERNAL_PRED_COLUMN_NAME].fillna(0)\n",
    "external_test_preds = merged_test_preds[EXTERNAL_PRED_COLUMN_NAME].values\n",
    "print(f\"External test predictions merged successfully. Shape: {external_test_preds.shape}\")\n",
    "\n",
    "\n",
    "# --- 2. Create the Final Meta-Dataset for the Test Set ---\n",
    "# The order MUST be identical to the training meta-dataset\n",
    "X_test_meta = np.column_stack((\n",
    "    test_preds_xgb, \n",
    "    test_preds_lgbm, \n",
    "    test_preds_lstm_att, \n",
    "    test_preds_cnn_multi, \n",
    "    test_preds_nn_multi_input,\n",
    "    external_test_preds\n",
    "))\n",
    "\n",
    "\n",
    "# --- 3. DIAGNOSE AND FIX NANS in the Final Stacked Array ---\n",
    "print(\"\\n--- Checking for NaNs in FINAL test predictions ---\")\n",
    "# Diagnostic: Find out which model is causing the problem on the test set\n",
    "model_names = ['XGB', 'LGBM', 'LSTM_Att', 'CNN_Multi', 'NN_Multi_Input', 'External_Model']\n",
    "for i, name in enumerate(model_names):\n",
    "    nan_count = np.isnan(X_test_meta[:, i]).sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"WARNING: Found {nan_count} NaN values in TEST predictions for model: {name}\")\n",
    "\n",
    "# --- THIS IS THE CRITICAL FIX ---\n",
    "# Check if any NaNs exist in the entire array and replace them with 0.\n",
    "# np.nan_to_num() is the most efficient way to do this for NumPy arrays.\n",
    "if np.isnan(X_test_meta).any():\n",
    "    print(\"Replacing NaN values with 0...\")\n",
    "    X_test_meta = np.nan_to_num(X_test_meta, nan=0.0)\n",
    "    print(\"NaNs replaced successfully.\")\n",
    "\n",
    "\n",
    "# --- 4. Make Final Predictions with the Cleaned Data ---\n",
    "print(\"\\nMaking final predictions with the meta-model...\")\n",
    "# This will now work because X_test_meta is guaranteed to be free of NaNs.\n",
    "final_predictions = meta_model.predict(X_test_meta)\n",
    "\n",
    "\n",
    "# --- 5. Create the Submission File ---\n",
    "print(\"\\nCreating submission.csv...\")\n",
    "submission_df = pd.DataFrame({'id': df_test[ID_COLUMN], 'price': final_predictions})\n",
    "submission_df['price'] = submission_df['price'].clip(0) # Final safety net\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file created successfully!\")\n",
    "print(\"Top 5 rows of submission.csv:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92063f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa77800e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
