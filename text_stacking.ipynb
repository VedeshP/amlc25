{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c51dc1e",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "- ### Models\n",
    "    - xgboost\n",
    "    - lightgbm\n",
    "    - LSTM\n",
    "    - Ridge Regression for met model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e767122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r /kaggle/input/requirements-amlc/req.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b56cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/amlc2025-dataset/sample_test.csv\n",
      "/kaggle/input/amlc2025-dataset/sample_test_out.csv\n",
      "/kaggle/input/amlc2025-dataset/train.csv\n",
      "/kaggle/input/amlc2025-dataset/test.csv\n",
      "/kaggle/input/requirements-amlc/req2.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7cdee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1502a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error # As a proxy for NN loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ab04e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe1cbb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 04:50:28.215325: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-12 04:50:28.236851: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-12 04:50:28.243326: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a9ddb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "346b129e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.1.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentence_transformers\n",
    "sentence_transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d60e7057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    N_SPLITS = 5\n",
    "    RANDOM_STATE = 42\n",
    "    DATA_PATH = '/kaggle/input/amlc2025-dataset/train.csv'\n",
    "    \n",
    "    # LSTM Config\n",
    "    LSTM_VOCAB_SIZE = 20000\n",
    "    LSTM_MAX_LEN = 60\n",
    "    LSTM_EMBEDDING_DIM = 100\n",
    "    \n",
    "    # Sentence Transformer Model Name\n",
    "    ST_MODEL_NAME = 'all-MiniLM-L6-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70964bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SMAPE metric function\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    ratio = np.where(denominator == 0, 0, numerator / denominator)\n",
    "    return np.mean(ratio) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8767885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CFG.DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0671d078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 75000 entries, 0 to 74999\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   sample_id        75000 non-null  int64  \n",
      " 1   catalog_content  75000 non-null  object \n",
      " 2   image_link       75000 non-null  object \n",
      " 3   price            75000 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dd29291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_engineered_features(df):\n",
    "    df_out = pd.DataFrame(index=df.index)\n",
    "    text_col = 'catalog_content'\n",
    "    \n",
    "    df_out[f'{text_col}_length'] = df[text_col].str.len()\n",
    "    df_out[f'{text_col}_word_count'] = df[text_col].str.split().str.len()\n",
    "    df_out[f'{text_col}_digit_count'] = df[text_col].apply(lambda t: sum(1 for c in t if c.isdigit()))\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99a7cd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating engineered features...\n",
      "Engineered features shape: (75000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating engineered features...\")\n",
    "engineered_features = create_engineered_features(df)\n",
    "print(f\"Engineered features shape: {engineered_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a47b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['catalog_content']]\n",
    "y = df['price'].values\n",
    "X_engineered = engineered_features.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c463ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_xgb_tfidf = np.zeros(len(df))\n",
    "oof_lgbm_st = np.zeros(len(df))\n",
    "oof_lstm = np.zeros(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5154d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=CFG.N_SPLITS, shuffle=True, random_state=CFG.RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebad77fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_model = SentenceTransformer(CFG.ST_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "104e6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# print(physical_devices)\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[1], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aad810ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FOLD 1 / 5 =====\n",
      "Training XGBoost on TF-IDF...\n",
      "Fold 1 XGB TF-IDF SMAPE: 62.9343%\n",
      "Training LightGBM on SentenceTransformer...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 98512\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 387\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 387 dense feature groups (22.20 MB) transferred to GPU in 0.022692 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 23.598634\n",
      "Fold 1 LGBM ST SMAPE: 70.2821%\n",
      "Training LSTM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760244840.744436     565 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1760244840.744932     565 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step\n",
      "Fold 1 LSTM SMAPE: 60.1558%\n",
      "\n",
      "===== FOLD 2 / 5 =====\n",
      "Training XGBoost on TF-IDF...\n",
      "Fold 2 XGB TF-IDF SMAPE: 62.1370%\n",
      "Training LightGBM on SentenceTransformer...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 98513\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 387\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 387 dense feature groups (22.20 MB) transferred to GPU in 0.021506 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 23.620979\n",
      "Fold 2 LGBM ST SMAPE: 69.5084%\n",
      "Training LSTM...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step\n",
      "Fold 2 LSTM SMAPE: 58.0344%\n",
      "\n",
      "===== FOLD 3 / 5 =====\n",
      "Training XGBoost on TF-IDF...\n",
      "Fold 3 XGB TF-IDF SMAPE: 62.7456%\n",
      "Training LightGBM on SentenceTransformer...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 98515\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 387\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 387 dense feature groups (22.20 MB) transferred to GPU in 0.022406 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 23.709702\n",
      "Fold 3 LGBM ST SMAPE: 69.8400%\n",
      "Training LSTM...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step\n",
      "Fold 3 LSTM SMAPE: 59.1530%\n",
      "\n",
      "===== FOLD 4 / 5 =====\n",
      "Training XGBoost on TF-IDF...\n",
      "Fold 4 XGB TF-IDF SMAPE: 61.9444%\n",
      "Training LightGBM on SentenceTransformer...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 98512\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 387\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 387 dense feature groups (22.20 MB) transferred to GPU in 0.022071 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 23.677128\n",
      "Fold 4 LGBM ST SMAPE: 68.9454%\n",
      "Training LSTM...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step\n",
      "Fold 4 LSTM SMAPE: 58.0725%\n",
      "\n",
      "===== FOLD 5 / 5 =====\n",
      "Training XGBoost on TF-IDF...\n",
      "Fold 5 XGB TF-IDF SMAPE: 62.6998%\n",
      "Training LightGBM on SentenceTransformer...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 98514\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 387\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 387 dense feature groups (22.20 MB) transferred to GPU in 0.022593 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 23.631827\n",
      "Fold 5 LGBM ST SMAPE: 69.6406%\n",
      "Training LSTM...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step\n",
      "Fold 5 LSTM SMAPE: 59.7605%\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"\\n===== FOLD {fold+1} / {CFG.N_SPLITS} =====\")\n",
    "    \n",
    "    # Split data for this fold\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    X_eng_train, X_eng_val = X_engineered[train_idx], X_engineered[val_idx]\n",
    "\n",
    "    # --- Model 1: XGBoost on TF-IDF + Engineered Features ---\n",
    "    print(\"Training XGBoost on TF-IDF...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=25000)\n",
    "    \n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['catalog_content'])\n",
    "    X_val_tfidf = tfidf_vectorizer.transform(X_val['catalog_content'])\n",
    "    \n",
    "    X_train_xgb = scipy.sparse.hstack((X_train_tfidf, X_eng_train))\n",
    "    X_val_xgb = scipy.sparse.hstack((X_val_tfidf, X_eng_val))\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(tree_method='hist', device='cuda', random_state=CFG.RANDOM_STATE)\n",
    "    xgb_model.fit(X_train_xgb, y_train)\n",
    "    \n",
    "    preds_xgb = xgb_model.predict(X_val_xgb)\n",
    "    oof_xgb_tfidf[val_idx] = preds_xgb\n",
    "    print(f\"Fold {fold+1} XGB TF-IDF SMAPE: {smape(y_val, preds_xgb):.4f}%\")\n",
    "    \n",
    "    # --- Model 2: LightGBM on SentenceTransformer + Engineered Features ---\n",
    "    print(\"Training LightGBM on SentenceTransformer...\")\n",
    "    X_train_st = st_model.encode(X_train['catalog_content'].tolist(), show_progress_bar=False)\n",
    "    X_val_st = st_model.encode(X_val['catalog_content'].tolist(), show_progress_bar=False)\n",
    "    \n",
    "    X_train_lgbm = np.hstack((X_train_st, X_eng_train))\n",
    "    X_val_lgbm = np.hstack((X_val_st, X_eng_val))\n",
    "    \n",
    "    lgbm_model = lgb.LGBMRegressor(device='gpu', random_state=CFG.RANDOM_STATE)\n",
    "    lgbm_model.fit(X_train_lgbm, y_train)\n",
    "    \n",
    "    preds_lgbm = lgbm_model.predict(X_val_lgbm)\n",
    "    oof_lgbm_st[val_idx] = preds_lgbm\n",
    "    print(f\"Fold {fold+1} LGBM ST SMAPE: {smape(y_val, preds_lgbm):.4f}%\")\n",
    "\n",
    "    # --- Model 3: LSTM with internal Embedding Layer ---\n",
    "    print(\"Training LSTM...\")\n",
    "    tokenizer = Tokenizer(num_words=CFG.LSTM_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train['catalog_content'])\n",
    "    \n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train['catalog_content'])\n",
    "    X_val_seq = tokenizer.texts_to_sequences(X_val['catalog_content'])\n",
    "    \n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=CFG.LSTM_MAX_LEN, padding='post', truncating='post')\n",
    "    X_val_pad = pad_sequences(X_val_seq, maxlen=CFG.LSTM_MAX_LEN, padding='post', truncating='post')\n",
    "    \n",
    "    lstm_model = Sequential([\n",
    "        Input(shape=(CFG.LSTM_MAX_LEN,)),\n",
    "        Embedding(input_dim=CFG.LSTM_VOCAB_SIZE, output_dim=CFG.LSTM_EMBEDDING_DIM),\n",
    "        # LSTM(64, return_sequences=False),\n",
    "        # changing the above line since due to a kaggle env issue cuDNN kernel not supported for the LSTM Layer\n",
    "        LSTM(64, return_sequences=False, activation='tanh'), \n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    lstm_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    lstm_model.fit(X_train_pad, y_train,\n",
    "                   validation_data=(X_val_pad, y_val),\n",
    "                   epochs=20,\n",
    "                   batch_size=128,\n",
    "                   callbacks=[early_stopping],\n",
    "                   verbose=0) # Set to 1 to see epoch progress\n",
    "    \n",
    "    preds_lstm = lstm_model.predict(X_val_pad, batch_size=512).squeeze()\n",
    "    oof_lstm[val_idx] = preds_lstm\n",
    "    print(f\"Fold {fold+1} LSTM SMAPE: {smape(y_val, preds_lstm):.4f}%\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3aaf474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Meta-Model ---\n",
      "Meta-model trained successfully.\n",
      "Final blended OOF predictions shape: (75000, 3)\n",
      "\n",
      "Overall OOF SMAPE of the full ensemble: 58.3435%\n",
      "Weights of the meta-model (XGB, LGBM, LSTM): [0.40070418 0.22180141 0.58282787]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Training Meta-Model ---\")\n",
    "X_meta = np.column_stack((oof_xgb_tfidf, oof_lgbm_st, oof_lstm))\n",
    "meta_model = Ridge(alpha=1.0)\n",
    "meta_model.fit(X_meta, y)\n",
    "print(\"Meta-model trained successfully.\")\n",
    "print(f\"Final blended OOF predictions shape: {X_meta.shape}\")\n",
    "final_oof_preds = meta_model.predict(X_meta)\n",
    "final_oof_smape = smape(y, final_oof_preds)\n",
    "print(f\"\\nOverall OOF SMAPE of the full ensemble: {final_oof_smape:.4f}%\")\n",
    "print(\"Weights of the meta-model (XGB, LGBM, LSTM):\", meta_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce96c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_PATH = '/kaggle/input/amlc2025-dataset/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cd7f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "590693e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = df_test['sample_id']\n",
    "X_test = df_test[['catalog_content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b928df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-training models on the full training dataset...\n",
      "Training final XGBoost model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Re-training models on the full training dataset...\")\n",
    "print(\"Training final XGBoost model...\")\n",
    "# Use the same vectorizer, but fit on ALL training data\n",
    "final_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=25000)\n",
    "full_train_tfidf = final_tfidf_vectorizer.fit_transform(df['catalog_content'])\n",
    "X_test_tfidf = final_tfidf_vectorizer.transform(X_test['catalog_content'])\n",
    "\n",
    "# Combine with engineered features\n",
    "full_train_engineered = create_engineered_features(df)\n",
    "test_engineered = create_engineered_features(df_test)\n",
    "full_train_xgb = scipy.sparse.hstack((full_train_tfidf, full_train_engineered.values))\n",
    "X_test_xgb = scipy.sparse.hstack((X_test_tfidf, test_engineered.values))\n",
    "\n",
    "# Train the final XGB model\n",
    "final_xgb_model = xgb.XGBRegressor(tree_method='hist', device='cuda', random_state=CFG.RANDOM_STATE)\n",
    "final_xgb_model.fit(full_train_xgb, y)\n",
    "test_preds_xgb = final_xgb_model.predict(X_test_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5947f6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final LightGBM model...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 98517\n",
      "[LightGBM] [Info] Number of data points in the train set: 75000, number of used features: 387\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 387 dense feature groups (27.75 MB) transferred to GPU in 0.026546 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 23.647654\n"
     ]
    }
   ],
   "source": [
    "# --- Model 2: LightGBM on SentenceTransformer ---\n",
    "print(\"Training final LightGBM model...\")\n",
    "full_train_st = st_model.encode(df['catalog_content'].tolist(), show_progress_bar=False)\n",
    "X_test_st = st_model.encode(X_test['catalog_content'].tolist(), show_progress_bar=False)\n",
    "\n",
    "full_train_lgbm = np.hstack((full_train_st, full_train_engineered.values))\n",
    "X_test_lgbm = np.hstack((X_test_st, test_engineered.values))\n",
    "\n",
    "# Train the final LGBM model\n",
    "final_lgbm_model = lgb.LGBMRegressor(device='gpu', random_state=CFG.RANDOM_STATE)\n",
    "final_lgbm_model.fit(full_train_lgbm, y)\n",
    "test_preds_lgbm = final_lgbm_model.predict(X_test_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c776c357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final LSTM model...\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step\n"
     ]
    }
   ],
   "source": [
    "# --- Model 3: LSTM ---\n",
    "print(\"Training final LSTM model...\")\n",
    "# Use the same tokenizer, but fit on ALL training data\n",
    "final_tokenizer = Tokenizer(num_words=CFG.LSTM_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "final_tokenizer.fit_on_texts(df['catalog_content'])\n",
    "\n",
    "full_train_seq = final_tokenizer.texts_to_sequences(df['catalog_content'])\n",
    "X_test_seq = final_tokenizer.texts_to_sequences(X_test['catalog_content'])\n",
    "\n",
    "full_train_pad = pad_sequences(full_train_seq, maxlen=CFG.LSTM_MAX_LEN, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=CFG.LSTM_MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "# Define and train the final LSTM model\n",
    "final_lstm_model = Sequential([\n",
    "    Input(shape=(CFG.LSTM_MAX_LEN,)),\n",
    "    Embedding(input_dim=CFG.LSTM_VOCAB_SIZE, output_dim=CFG.LSTM_EMBEDDING_DIM),\n",
    "    LSTM(64, return_sequences=False, activation='tanh'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "final_lstm_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# Note: No validation data here, as we use all data for training\n",
    "final_lstm_model.fit(full_train_pad, y, epochs=10, batch_size=128, verbose=0) # Train for a reasonable number of epochs\n",
    "test_preds_lstm = final_lstm_model.predict(X_test_pad, batch_size=512).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08d00e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making final predictions with the meta-model...\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Make Final Predictions with the Meta-Model ---\n",
    "print(\"Making final predictions with the meta-model...\")\n",
    "# Stack the test predictions from the base models\n",
    "X_test_meta = np.column_stack((test_preds_xgb, test_preds_lgbm, test_preds_lstm))\n",
    "\n",
    "# Use the already-trained meta_model to predict on the test meta-features\n",
    "final_predictions = meta_model.predict(X_test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6c6c24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission.csv...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating submission.csv...\")\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_id': test_ids,\n",
    "    'price': final_predictions\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "795be9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission file created successfully!\n",
      "Top 5 rows of submission.csv:\n",
      "   sample_id      price\n",
      "0     100179  20.975271\n",
      "1     245611  12.173563\n",
      "2     146263  32.372255\n",
      "3      95658  11.906809\n",
      "4      36806  96.555306\n"
     ]
    }
   ],
   "source": [
    "submission_df['price'] = submission_df['price'].clip(0)\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file created successfully!\")\n",
    "print(\"Top 5 rows of submission.csv:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5567b130",
   "metadata": {},
   "source": [
    "kaggle environment\n",
    "\n",
    "use tf 2.17.1\n",
    "and\n",
    "keras 3.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "094367bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "print(type(test_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe8a1bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "print(type(final_predictions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede9437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
