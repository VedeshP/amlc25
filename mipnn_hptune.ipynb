{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3338bb3a",
   "metadata": {},
   "source": [
    "Notebook will be run on kaggle non interactive session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6307c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import gc\n",
    "import optuna\n",
    "\n",
    "# Scikit-learn, Boosters, TF, Transformers, FAISS...\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Concatenate, Dropout, Bidirectional, LSTM, GRU\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad2a4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    N_SPLITS = 5\n",
    "    RANDOM_STATE = 42\n",
    "    DATA_PATH = '/kaggle/input/amlc2025-dataset/train.csv'\n",
    "    \n",
    "    # --- Text Embedding Models ---\n",
    "    ST_MODELS = [\n",
    "        'sentence-transformers/all-MiniLM-L12-v2',\n",
    "        'BAAI/bge-base-en-v1.5',\n",
    "        'sentence-transformers/all-distilroberta-v1',\n",
    "        'sentence-transformers/paraphrase-mpnet-base-v2'\n",
    "    ]\n",
    "    BGE_MODEL_INDEX = 1\n",
    "    \n",
    "    # --- Feature Generation Config ---\n",
    "    KNN_N_NEIGHBORS = 10\n",
    "    \n",
    "    # --- Deep Learning Model Config ---\n",
    "    DL_VOCAB_SIZE = 30000\n",
    "    DL_MAX_LEN = 60\n",
    "\n",
    "    # --- Default Hyperparameters (Placeholders for Tuning) ---\n",
    "    LGBM_PARAMS = {'objective': 'regression_l1', 'metric': 'mae', 'n_estimators': 2000, 'learning_rate': 0.05, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'num_leaves': 31, 'verbose': -1, 'n_jobs': -1, 'seed': 42}\n",
    "    XGB_PARAMS = {'objective': 'reg:squarederror', 'eval_metric': 'mae', 'n_estimators': 2000, 'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.8, 'colsample_bytree': 0.8, 'tree_method': 'gpu_hist', 'seed': 42}\n",
    "    LSTM_ATT_PARAMS = {'embedding_dim': 128, 'lstm_units': 64, 'dense_units': 64, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
    "    CNN_MULTI_PARAMS = {'embedding_dim': 128, 'cnn_filters': 64, 'dense_units': 128, 'dropout_rate': 0.5, 'learning_rate': 0.001}\n",
    "    NN_MULTI_INPUT_PARAMS = {'embedding_dim': 64, 'gru_units': 64, 'text_branch_dense_units': 32, 'tabular_branch_dense_units': 32, 'final_dense_units': 64, 'dropout_rate': 0.4, 'learning_rate': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df69029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SMAPE metric function\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    ratio = np.where(denominator == 0, 0, numerator / denominator)\n",
    "    return np.mean(ratio) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a35ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CFG.DATA_PATH)\n",
    "y = df['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92e85a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extreme_engineered_features(df):\n",
    "    df_out = pd.DataFrame(index=df.index)\n",
    "    text_col = 'catalog_content'\n",
    "    \n",
    "    # Meta Features\n",
    "    df_out[f'{text_col}_length'] = df[text_col].str.len()\n",
    "    df_out[f'{text_col}_word_count'] = df[text_col].str.split().str.len()\n",
    "    df_out[f'{text_col}_capital_ratio'] = df[text_col].apply(lambda t: sum(1 for c in t if c.isupper()) / (len(t) + 1e-9))\n",
    "    \n",
    "    # Entity Extraction (Example lists - expand these!)\n",
    "    BRANDS = ['sony', 'samsung', 'nike', 'apple', 'kitchenaid']\n",
    "    MATERIALS = ['cotton', 'leather', 'silk', 'gold', 'nylon', 'polyester', 'silver']\n",
    "    \n",
    "    text_lower = df[text_col].str.lower()\n",
    "    df_out['has_brand'] = text_lower.apply(lambda t: any(brand in t for brand in BRANDS)).astype(int)\n",
    "    for material in MATERIALS:\n",
    "        df_out[f'has_{material}'] = text_lower.str.contains(material).astype(int)\n",
    "        \n",
    "    # Regex Features\n",
    "    df_out['extracted_inch'] = text_lower.str.extract(r'(\\d+\\.?\\d*)\\s*(inch|\\\"|in\\b)').iloc[:, 0].astype(float)\n",
    "    df_out['extracted_gb'] = text_lower.str.extract(r'(\\d+)\\s*gb').iloc[:, 0].astype(float)\n",
    "    \n",
    "    # Fill NaNs from regex with a neutral value like 0\n",
    "    return df_out.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e2cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transformer_embeddings(df, models):\n",
    "    all_embeddings = []\n",
    "    for model_name in models:\n",
    "        print(f\"   Generating embeddings for: {model_name}\")\n",
    "        model = SentenceTransformer(model_name)\n",
    "        embeddings = model.encode(df['catalog_content'].tolist(), show_progress_bar=True)\n",
    "        all_embeddings.append(embeddings)\n",
    "    return np.concatenate(all_embeddings, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae0d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_knn_features(index_embeds, y_index, query_embeds, k):\n",
    "    \"\"\"\n",
    "    Generates KNN features.\n",
    "    'index_embeds' are used to build the search space.\n",
    "    'query_embeds' are the items we want to find neighbors for.\n",
    "    \"\"\"\n",
    "    d = index_embeds.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    \n",
    "    # Check if GPU is available\n",
    "    try:\n",
    "        res = faiss.StandardGpuResources()\n",
    "        gpu_index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "        gpu_index.add(index_embeds.astype(np.float32))\n",
    "        search_index = gpu_index\n",
    "    except AttributeError:\n",
    "        # Fallback to CPU if faiss-gpu is not installed or fails\n",
    "        print(\"   FAISS-GPU not found or failed, falling back to CPU.\")\n",
    "        index.add(index_embeds.astype(np.float32))\n",
    "        search_index = index\n",
    "\n",
    "    # If query is the same as index, we are doing self-search. K+1 and ignore first result.\n",
    "    is_self_search = np.array_equal(index_embeds, query_embeds)\n",
    "    if is_self_search:\n",
    "        _, I = search_index.search(query_embeds.astype(np.float32), k + 1)\n",
    "        I = I[:, 1:] # Exclude the first column which is the item itself\n",
    "    else:\n",
    "        _, I = search_index.search(query_embeds.astype(np.float32), k)\n",
    "    \n",
    "    neighbor_prices = y_index[I]\n",
    "    \n",
    "    knn_feats = np.zeros((len(query_embeds), 3))\n",
    "    knn_feats[:, 0] = np.mean(neighbor_prices, axis=1)\n",
    "    knn_feats[:, 1] = np.median(neighbor_prices, axis=1)\n",
    "    knn_feats[:, 2] = np.std(neighbor_prices, axis=1)\n",
    "    \n",
    "    return knn_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b08c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating all features...\")\n",
    "extreme_engineered_features = create_extreme_engineered_features(df)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=50000)\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(df['catalog_content'])\n",
    "combined_embeddings = generate_transformer_embeddings(df, CFG.ST_MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eb5ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Splitting data to get a single fold for tuning...\")\n",
    "kf = KFold(n_splits=CFG.N_SPLITS, shuffle=True, random_state=CFG.RANDOM_STATE)\n",
    "train_idx, val_idx = next(iter(kf.split(df))) # Get the first fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ade6ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split all feature sets\n",
    "X_train_text, X_val_text = df['catalog_content'].iloc[train_idx], df['catalog_content'].iloc[val_idx]\n",
    "y_train, y_val = y[train_idx], y[val_idx]\n",
    "X_train_eng, X_val_eng = extreme_engineered_features.iloc[train_idx].values, extreme_engineered_features.iloc[val_idx].values\n",
    "X_train_tfidf, X_val_tfidf = tfidf_features[train_idx], tfidf_features[val_idx]\n",
    "X_train_embed, X_val_embed = combined_embeddings[train_idx], combined_embeddings[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4ee884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate KNN features for the tuning fold\n",
    "model_dims = [SentenceTransformer(m).get_sentence_embedding_dimension() for m in CFG.ST_MODELS]\n",
    "bge_start_index = sum(model_dims[:CFG.BGE_MODEL_INDEX])\n",
    "bge_end_index = bge_start_index + model_dims[CFG.BGE_MODEL_INDEX]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e869e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_train_embeds = X_train_embed[:, bge_start_index:bge_end_index]\n",
    "bge_val_embeds = X_val_embed[:, bge_start_index:bge_end_index]\n",
    "knn_feats_train = generate_knn_features(bge_train_embeds, y_train, bge_train_embeds, CFG.KNN_N_NEIGHBORS)\n",
    "knn_feats_val = generate_knn_features(bge_train_embeds, y_train, bge_val_embeds, CFG.KNN_N_NEIGHBORS)\n",
    "\n",
    "print(\"\\n--- COMMON SETUP COMPLETE. ALL DATA FOR TUNING IS READY. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a05a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom Attention layer\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
    "        super(Attention, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        et = tf.keras.backend.squeeze(tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b), axis=-1)\n",
    "        at = tf.keras.backend.softmax(et)\n",
    "        at = tf.keras.backend.expand_dims(at, axis=-1)\n",
    "        output = x * at\n",
    "        return tf.keras.backend.sum(output, axis=1)\n",
    "\n",
    "# Model 3: LSTM with Attention\n",
    "def create_lstm_attention_model(vocab_size, max_len, embedding_dim):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    embedding = Embedding(vocab_size, embedding_dim)(inputs)\n",
    "    bilstm = Bidirectional(LSTM(128, return_sequences=True))(embedding)\n",
    "    attention = Attention()(bilstm)\n",
    "    dense = Dense(64, activation='relu')(attention)\n",
    "    outputs = Dense(1)(dense)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Model 4: 1D CNN with Multiple Kernels\n",
    "def create_multikernel_cnn_model(vocab_size, max_len, embedding_dim):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    embedding = Embedding(vocab_size, embedding_dim)(inputs)\n",
    "    \n",
    "    conv_layers = []\n",
    "    for kernel_size in [2, 3, 5]:\n",
    "        conv = Conv1D(filters=64, kernel_size=kernel_size, activation='relu')(embedding)\n",
    "        pool = GlobalMaxPooling1D()(conv)\n",
    "        conv_layers.append(pool)\n",
    "    \n",
    "    concatenated = Concatenate()(conv_layers)\n",
    "    dense = Dense(128, activation='relu')(concatenated)\n",
    "    dropout = Dropout(0.5)(dense)\n",
    "    outputs = Dense(1)(dropout)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Model 5: Multi-Input Neural Network\n",
    "def create_multi_input_model(vocab_size, max_len, embedding_dim, num_tabular_feats):\n",
    "    # Text input branch\n",
    "    text_input = Input(shape=(max_len,), name='text_input')\n",
    "    embedding = Embedding(vocab_size, embedding_dim)(text_input)\n",
    "    gru = GRU(64)(embedding)\n",
    "    text_branch = Dense(32, activation='relu')(gru)\n",
    "    \n",
    "    # Tabular input branch\n",
    "    tabular_input = Input(shape=(num_tabular_feats,), name='tabular_input')\n",
    "    tabular_branch = Dense(32, activation='relu')(tabular_input)\n",
    "    \n",
    "    # Merge branches\n",
    "    concatenated = Concatenate()([text_branch, tabular_branch])\n",
    "    dense = Dense(64, activation='relu')(concatenated)\n",
    "    outputs = Dense(1)(dense)\n",
    "    \n",
    "    return Model(inputs=[text_input, tabular_input], outputs=outputs)\n",
    "\n",
    "print(\"New model architectures are defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833c23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tokenize text for all DL models ---\n",
    "tokenizer = Tokenizer(num_words=CFG.DL_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train_text), maxlen=CFG.DL_MAX_LEN)\n",
    "X_val_seq = pad_sequences(tokenizer.texts_to_sequences(X_val_text), maxlen=CFG.DL_MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f3c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Assemble tabular features for Multi-Input NN ---\n",
    "tabular_train_feats = np.hstack([X_train_eng, knn_feats_train])\n",
    "tabular_val_feats = np.hstack([X_val_eng, knn_feats_val])\n",
    "\n",
    "def nn_multi_input_objective(trial):\n",
    "    params = {\n",
    "        'embedding_dim': trial.suggest_categorical('embedding_dim', [64, 128]),\n",
    "        'gru_units': trial.suggest_categorical('gru_units', [64, 128]),\n",
    "        'text_branch_dense_units': trial.suggest_categorical('text_branch_dense_units', [32, 64]),\n",
    "        'tabular_branch_dense_units': trial.suggest_categorical('tabular_branch_dense_units', [32, 64]),\n",
    "        'final_dense_units': trial.suggest_categorical('final_dense_units', [64, 128]),\n",
    "        'dropout_rate': trial.suggest_float('dropout_rate', 0.2, 0.6),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    }\n",
    "    model = create_multi_input_model(CFG.DL_VOCAB_SIZE, CFG.DL_MAX_LEN, tabular_train_feats.shape[1], **params)\n",
    "    model.compile(optimizer=Adam(params['learning_rate']), loss='mean_squared_error')\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit([X_train_seq, tabular_train_feats], y_train, validation_data=([X_val_seq, tabular_val_feats], y_val), epochs=30, batch_size=256, callbacks=[es], verbose=0)\n",
    "    preds = model.predict([X_val_seq, tabular_val_feats], batch_size=512).squeeze()\n",
    "    return smape(y_val, preds)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(nn_multi_input_objective, n_trials=20)\n",
    "print(\"\\nBest Multi-Input NN Parameters:\", study.best_params)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
