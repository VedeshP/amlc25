{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e5cd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/requirements-amlc/req2.txt\n",
      "/kaggle/input/amlc2025-dataset/sample_test.csv\n",
      "/kaggle/input/amlc2025-dataset/sample_test_out.csv\n",
      "/kaggle/input/amlc2025-dataset/train.csv\n",
      "/kaggle/input/amlc2025-dataset/test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a793f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c0a17c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa5e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16a587ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 09:27:18.696434: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-12 09:27:18.718090: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-12 09:27:18.724684: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a812aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14d0a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "868f65cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    N_SPLITS = 5\n",
    "    RANDOM_STATE = 42\n",
    "    DATA_PATH = '/kaggle/input/amlc2025-dataset/train.csv'\n",
    "    TEST_DATA_PATH = '/kaggle/input/amlc2025-dataset/test.csv'\n",
    "    \n",
    "    # Text Embedding Models\n",
    "    # Using a diverse pair: one for general semantics, one optimized for similarity\n",
    "    ST_MODELS = [\n",
    "        'sentence-transformers/all-MiniLM-L12-v2',\n",
    "        'BAAI/bge-base-en-v1.5',                     \n",
    "        'sentence-transformers/all-distilroberta-v1',\n",
    "        'sentence-transformers/paraphrase-mpnet-base-v2'\n",
    "    ]\n",
    "    BGE_MODEL_INDEX = 1 # The BGE model is the second in the list, at index 1\n",
    "    \n",
    "    # KNN Feature Generation\n",
    "    KNN_N_NEIGHBORS = 10\n",
    "    \n",
    "    # DenseNet (1D CNN) Config\n",
    "    CNN_VOCAB_SIZE = 30000\n",
    "    CNN_MAX_LEN = 60\n",
    "    CNN_EMBEDDING_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19894c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CFG.DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "323e4397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SMAPE metric function\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    ratio = np.where(denominator == 0, 0, numerator / denominator)\n",
    "    return np.mean(ratio) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb833f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_engineered_features(df):\n",
    "    df_out = pd.DataFrame(index=df.index)\n",
    "    text_col = 'catalog_content'\n",
    "    df_out[f'{text_col}_length'] = df[text_col].str.len()\n",
    "    df_out[f'{text_col}_word_count'] = df[text_col].str.split().str.len()\n",
    "    df_out[f'{text_col}_digit_count'] = df[text_col].apply(lambda t: sum(1 for c in t if c.isdigit()))\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53de4efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3: Creating engineered features...\n",
      "Engineered features shape: (75000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"1/3: Creating engineered features...\")\n",
    "engineered_features = create_engineered_features(df)\n",
    "print(f\"Engineered features shape: {engineered_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "690b59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transformer_embeddings(df, models):\n",
    "    all_embeddings = []\n",
    "    for model_name in models:\n",
    "        print(f\"   Generating embeddings for: {model_name}\")\n",
    "        model = SentenceTransformer(model_name)\n",
    "        embeddings = model.encode(df['catalog_content'].tolist(), show_progress_bar=True)\n",
    "        all_embeddings.append(embeddings)\n",
    "    return np.concatenate(all_embeddings, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c152866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/3: Creating text embeddings...\n",
      "   Generating embeddings for: sentence-transformers/all-MiniLM-L12-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf2695bf1684bd481b59b9815d1a453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Generating embeddings for: BAAI/bge-base-en-v1.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc9d3ed11b14e5abceb31f812370836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8803ea9a94034d73ac2d00888419698b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d1ebe861e14828a25e5cf7e0e41bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5788413381354151b1dda7b6ebc0cf74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142d7f08ab5049d59564d15084ff87a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f587f1736744c82a8d22eb5acb5e3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd0ca1c73a64788891d26d304e680f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a3358ce054440b880aa6b4b9c63c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd35a1bd319438c870a0fc21e5aefd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc422922daf64c7e8e17b8a64290a938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c7c127eb93462c80cb7c72002dfad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b6d8c2cedf4ae7a5a11b15f8742d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Generating embeddings for: sentence-transformers/all-distilroberta-v1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f00e981234e4c6b86b0fab65b7b9e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121639b047ff4d91be260ece081a4d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bbdde7635854a298edf29f14034213f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62320f09fad64779876961e08185bb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2dd90c7b2442ae970191cc53a75fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fa94d012fd44879d9121ee96b4de70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d81c78f3dd446cb079ebda4bca3d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb323af257174caf8aa4cd89e367bdc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1a5c3e70ff4e5c89e78e363e03af71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f71a54890042a7a19a992e02dfc7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b79a17476049748e594c16fac81898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97fa4a2b46b54ace8e29293efabf75b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3738aa881594cf4accb97691a488063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Generating embeddings for: sentence-transformers/paraphrase-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16f79528e694fa79cc2e7bc4a38391f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a9beebec9847868e54c1b4f5ebde38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544cf6242a8e461a9411535663f12356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396d0ae7133945688d7a4b7cfe346f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8852744ee5bc43a4815a7a71b0cf6344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/594 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f527657e7949e0a3508ad0db816785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78368cd18ee148d6bf9243f6c6da7b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8260f167645d483abd37485e5e35ce7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3817cf0385374864828d2c97f16d1ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f3d023ad8e4198aa78d074f4e3ac6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e90041fcf104469a6cc382d2b23c227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d66e5373d004394b4c7b9ccd7fffdf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined text embeddings shape: (75000, 2688)\n"
     ]
    }
   ],
   "source": [
    "print(\"2/3: Creating text embeddings...\")\n",
    "combined_embeddings = generate_transformer_embeddings(df, CFG.ST_MODELS)\n",
    "print(f\"Combined text embeddings shape: {combined_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eceb0aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3: KNN feature generation function is ready.\n"
     ]
    }
   ],
   "source": [
    "# Replace your old KNN function with this improved one.\n",
    "# It's now located in SECTION 4 for clarity.\n",
    "\n",
    "# in SECTION 4.3\n",
    "def generate_knn_features(index_embeds, y_index, query_embeds, k):\n",
    "    \"\"\"\n",
    "    Generates KNN features.\n",
    "    'index_embeds' are used to build the search space.\n",
    "    'query_embeds' are the items we want to find neighbors for.\n",
    "    \"\"\"\n",
    "    d = index_embeds.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    \n",
    "    # Check if GPU is available\n",
    "    try:\n",
    "        res = faiss.StandardGpuResources()\n",
    "        gpu_index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "        gpu_index.add(index_embeds.astype(np.float32))\n",
    "        search_index = gpu_index\n",
    "    except AttributeError:\n",
    "        # Fallback to CPU if faiss-gpu is not installed or fails\n",
    "        print(\"   FAISS-GPU not found or failed, falling back to CPU.\")\n",
    "        index.add(index_embeds.astype(np.float32))\n",
    "        search_index = index\n",
    "\n",
    "    # If query is the same as index, we are doing self-search. K+1 and ignore first result.\n",
    "    is_self_search = np.array_equal(index_embeds, query_embeds)\n",
    "    if is_self_search:\n",
    "        _, I = search_index.search(query_embeds.astype(np.float32), k + 1)\n",
    "        I = I[:, 1:] # Exclude the first column which is the item itself\n",
    "    else:\n",
    "        _, I = search_index.search(query_embeds.astype(np.float32), k)\n",
    "    \n",
    "    neighbor_prices = y_index[I]\n",
    "    \n",
    "    knn_feats = np.zeros((len(query_embeds), 3))\n",
    "    knn_feats[:, 0] = np.mean(neighbor_prices, axis=1)\n",
    "    knn_feats[:, 1] = np.median(neighbor_prices, axis=1)\n",
    "    knn_feats[:, 2] = np.std(neighbor_prices, axis=1)\n",
    "    \n",
    "    return knn_feats\n",
    "\n",
    "print(\"3/3: KNN feature generation function is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef62c37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 2: Training Base Models with Cross-Validation ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Phase 2: Training Base Models with Cross-Validation ---\")\n",
    "\n",
    "# Prepare data\n",
    "X_text = df['catalog_content']\n",
    "y = df['price'].values\n",
    "X_engineered = engineered_features.values\n",
    "\n",
    "# OOF arrays\n",
    "oof_xgb = np.zeros(len(df))\n",
    "oof_lgbm = np.zeros(len(df))\n",
    "oof_densenet = np.zeros(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5687c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=CFG.N_SPLITS, shuffle=True, random_state=CFG.RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e93c4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating embedding dimensions for KNN slicing...\n",
      "BGE embeddings will be sliced from column 384 to 1152.\n",
      "\n",
      "===== FOLD 1 / 5 =====\n",
      "1/3: Training XGBoost...\n",
      "   Fold 1 XGB SMAPE: 63.0302%\n",
      "2/3: Training LightGBM...\n",
      "   Generating KNN features for training set...\n",
      "   Generating KNN features for validation set...\n",
      "   Train features shape: (60000, 2694), Val features shape: (15000, 2694)\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 686797\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2694\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 2694 dense feature groups (154.27 MB) transferred to GPU in 0.145207 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 23.598634\n",
      "   Fold 1 LGBM SMAPE: 62.5634%\n",
      "3/3: Training DenseNet (1D CNN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760266008.065772     253 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1760266008.066434     253 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "   Fold 1 DenseNet SMAPE: 61.0471%\n",
      "\n",
      "===== FOLD 2 / 5 =====\n",
      "1/3: Training XGBoost...\n",
      "   Fold 2 XGB SMAPE: 62.0947%\n",
      "2/3: Training LightGBM...\n",
      "   Generating KNN features for training set...\n",
      "   Generating KNN features for validation set...\n",
      "   Train features shape: (60000, 2694), Val features shape: (15000, 2694)\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 686798\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2694\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 2694 dense feature groups (154.27 MB) transferred to GPU in 0.143671 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 23.620979\n",
      "   Fold 2 LGBM SMAPE: 61.8145%\n",
      "3/3: Training DenseNet (1D CNN)...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "   Fold 2 DenseNet SMAPE: 61.2779%\n",
      "\n",
      "===== FOLD 3 / 5 =====\n",
      "1/3: Training XGBoost...\n",
      "   Fold 3 XGB SMAPE: 62.6559%\n",
      "2/3: Training LightGBM...\n",
      "   Generating KNN features for training set...\n",
      "   Generating KNN features for validation set...\n",
      "   Train features shape: (60000, 2694), Val features shape: (15000, 2694)\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 686800\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2694\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 2694 dense feature groups (154.27 MB) transferred to GPU in 0.145186 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 23.709702\n",
      "   Fold 3 LGBM SMAPE: 62.1011%\n",
      "3/3: Training DenseNet (1D CNN)...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "   Fold 3 DenseNet SMAPE: 61.7993%\n",
      "\n",
      "===== FOLD 4 / 5 =====\n",
      "1/3: Training XGBoost...\n",
      "   Fold 4 XGB SMAPE: 61.7639%\n",
      "2/3: Training LightGBM...\n",
      "   Generating KNN features for training set...\n",
      "   Generating KNN features for validation set...\n",
      "   Train features shape: (60000, 2694), Val features shape: (15000, 2694)\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 686797\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2694\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 2694 dense feature groups (154.27 MB) transferred to GPU in 0.147621 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 23.677128\n",
      "   Fold 4 LGBM SMAPE: 61.1684%\n",
      "3/3: Training DenseNet (1D CNN)...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "   Fold 4 DenseNet SMAPE: 60.4890%\n",
      "\n",
      "===== FOLD 5 / 5 =====\n",
      "1/3: Training XGBoost...\n",
      "   Fold 5 XGB SMAPE: 62.5320%\n",
      "2/3: Training LightGBM...\n",
      "   Generating KNN features for training set...\n",
      "   Generating KNN features for validation set...\n",
      "   Train features shape: (60000, 2694), Val features shape: (15000, 2694)\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 686799\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2694\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 2694 dense feature groups (154.27 MB) transferred to GPU in 0.145744 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 23.631827\n",
      "   Fold 5 LGBM SMAPE: 61.8610%\n",
      "3/3: Training DenseNet (1D CNN)...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "   Fold 5 DenseNet SMAPE: 60.6404%\n"
     ]
    }
   ],
   "source": [
    "# Pre-calculate the dimensions of each sentence transformer model to find the BGE slice\n",
    "print(\"Calculating embedding dimensions for KNN slicing...\")\n",
    "model_dims = [SentenceTransformer(m).get_sentence_embedding_dimension() for m in CFG.ST_MODELS]\n",
    "bge_start_index = sum(model_dims[:CFG.BGE_MODEL_INDEX])\n",
    "bge_end_index = bge_start_index + model_dims[CFG.BGE_MODEL_INDEX]\n",
    "print(f\"BGE embeddings will be sliced from column {bge_start_index} to {bge_end_index}.\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "    print(f\"\\n===== FOLD {fold+1} / {CFG.N_SPLITS} =====\")\n",
    "    \n",
    "    # --- Split Data for this Fold ---\n",
    "    X_train_text, X_val_text = X_text.iloc[train_idx], X_text.iloc[val_idx]\n",
    "    X_train_eng, X_val_eng = X_engineered[train_idx], X_engineered[val_idx]\n",
    "    X_train_embed, X_val_embed = combined_embeddings[train_idx], combined_embeddings[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # --- Model 1: XGBoost (Keyword Expert) ---\n",
    "    print(\"1/3: Training XGBoost...\")\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1, 2), max_features=30000)\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "    X_val_tfidf = tfidf.transform(X_val_text)\n",
    "    \n",
    "    X_train_xgb = scipy.sparse.hstack((X_train_tfidf, X_train_eng))\n",
    "    X_val_xgb = scipy.sparse.hstack((X_val_tfidf, X_val_eng))\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(tree_method='hist', device='cuda', random_state=CFG.RANDOM_STATE)\n",
    "    xgb_model.fit(X_train_xgb, y_train)\n",
    "    oof_xgb[val_idx] = xgb_model.predict(X_val_xgb)\n",
    "    print(f\"   Fold {fold+1} XGB SMAPE: {smape(y_val, oof_xgb[val_idx]):.4f}%\")\n",
    "    # trained on tfidf + engineered features\n",
    "\n",
    "    # --- Model 2: LightGBM (Semantic & Market Expert) ---\n",
    "    print(\"2/3: Training LightGBM...\")\n",
    "\n",
    "    # Slice out the BGE embeddings, which are best for similarity search, using our pre-calculated indices\n",
    "    bge_train_embeds = X_train_embed[:, bge_start_index:bge_end_index]\n",
    "    bge_val_embeds = X_val_embed[:, bge_start_index:bge_end_index]\n",
    "\n",
    "    # Generate KNN features for BOTH the training and validation sets\n",
    "    # This ensures the number of features is identical for both fit() and predict()\n",
    "    print(\"   Generating KNN features for training set...\")\n",
    "    # For the training set, we find neighbors within itself\n",
    "    knn_feats_train = generate_knn_features(bge_train_embeds, y_train, bge_train_embeds, CFG.KNN_N_NEIGHBORS)\n",
    "\n",
    "    print(\"   Generating KNN features for validation set...\")\n",
    "    # For the validation set, we find neighbors from the training set (leakage-free for OOF)\n",
    "    knn_feats_val = generate_knn_features(bge_train_embeds, y_train, bge_val_embeds, CFG.KNN_N_NEIGHBORS)\n",
    "\n",
    "    # Assemble the final feature matrices, now with identical structures\n",
    "    X_train_lgbm = np.hstack([X_train_embed, X_train_eng, knn_feats_train])\n",
    "    X_val_lgbm = np.hstack([X_val_embed, X_val_eng, knn_feats_val])\n",
    "    \n",
    "    # This print statement is a great sanity check to confirm the fix\n",
    "    print(f\"   Train features shape: {X_train_lgbm.shape}, Val features shape: {X_val_lgbm.shape}\")\n",
    "    \n",
    "    # Train the model and make predictions\n",
    "    lgbm_model = lgb.LGBMRegressor(device='gpu', random_state=CFG.RANDOM_STATE)\n",
    "    lgbm_model.fit(X_train_lgbm, y_train)\n",
    "    oof_lgbm[val_idx] = lgbm_model.predict(X_val_lgbm)\n",
    "    print(f\"   Fold {fold+1} LGBM SMAPE: {smape(y_val, oof_lgbm[val_idx]):.4f}%\")\n",
    "    \n",
    "    # --- Model 3: DenseNet (Deep Learning Specialist) ---\n",
    "    print(\"3/3: Training DenseNet (1D CNN)...\")\n",
    "    tokenizer = Tokenizer(num_words=CFG.CNN_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train_text)\n",
    "    \n",
    "    X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train_text), maxlen=CFG.CNN_MAX_LEN)\n",
    "    X_val_seq = pad_sequences(tokenizer.texts_to_sequences(X_val_text), maxlen=CFG.CNN_MAX_LEN)\n",
    "\n",
    "    # Simple 1D CNN model\n",
    "    input_text = Input(shape=(CFG.CNN_MAX_LEN,))\n",
    "    embedding = Embedding(CFG.CNN_VOCAB_SIZE, CFG.CNN_EMBEDDING_DIM)(input_text)\n",
    "    conv1 = Conv1D(filters=64, kernel_size=3, activation='relu')(embedding)\n",
    "    pool1 = GlobalMaxPooling1D()(conv1)\n",
    "    dense1 = Dense(64, activation='relu')(pool1)\n",
    "    dropout1 = Dropout(0.5)(dense1)\n",
    "    output = Dense(1)(dropout1)\n",
    "    cnn_model = Model(inputs=input_text, outputs=output)\n",
    "    cnn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    cnn_model.fit(X_train_seq, y_train, validation_data=(X_val_seq, y_val),\n",
    "                  epochs=20, batch_size=128, callbacks=[es], verbose=0)\n",
    "    \n",
    "    oof_densenet[val_idx] = cnn_model.predict(X_val_seq, batch_size=512).squeeze()\n",
    "    print(f\"   Fold {fold+1} DenseNet SMAPE: {smape(y_val, oof_densenet[val_idx]):.4f}%\")\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7ea0623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 3: Training Meta-Model ---\n",
      "\n",
      "Overall OOF SMAPE of the full ensemble: 57.1667%\n",
      "Meta-Model Weights (XGB, LGBM, DenseNet): [0.38743498 0.45836708 0.36707637]\n",
      "\n",
      "Training pipeline complete. Ready for final model training and submission generation.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Phase 3: Training Meta-Model ---\")\n",
    "\n",
    "# Create the training data for the meta-model\n",
    "X_meta = np.column_stack((oof_xgb, oof_lgbm, oof_densenet))\n",
    "\n",
    "# Train the meta-model\n",
    "meta_model = Ridge(alpha=1.0)\n",
    "meta_model.fit(X_meta, y)\n",
    "\n",
    "# Evaluate the final OOF predictions from the blended model\n",
    "final_oof_preds = meta_model.predict(X_meta)\n",
    "final_oof_smape = smape(y, final_oof_preds)\n",
    "\n",
    "print(f\"\\nOverall OOF SMAPE of the full ensemble: {final_oof_smape:.4f}%\")\n",
    "print(f\"Meta-Model Weights (XGB, LGBM, DenseNet): {meta_model.coef_}\")\n",
    "print(\"\\nTraining pipeline complete. Ready for final model training and submission generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51d6f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(CFG.TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c639cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = df_test['sample_id'] \n",
    "X_test_text = df_test['catalog_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6daf5e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/4: Generating features for the full train and test sets...\n"
     ]
    }
   ],
   "source": [
    "print(\"1/4: Generating features for the full train and test sets...\")\n",
    "full_train_engineered = create_engineered_features(df)\n",
    "test_engineered = create_engineered_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a98659a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=30000)\n",
    "full_train_tfidf = final_tfidf_vectorizer.fit_transform(df['catalog_content'])\n",
    "X_test_tfidf = final_tfidf_vectorizer.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d57b36e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Generating embeddings for test set...\n",
      "   Generating embeddings for: sentence-transformers/all-MiniLM-L12-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0785ca302549bdbeab500a38128b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Generating embeddings for: BAAI/bge-base-en-v1.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0220aedacf64fb78de09c3ddd3c7212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Generating embeddings for: sentence-transformers/all-distilroberta-v1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f772a76a979743aa94b77eb36f9e034f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Generating embeddings for: sentence-transformers/paraphrase-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6188251f9d03457ea62c3fa0652da51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"   Generating embeddings for test set...\")\n",
    "test_embeddings = generate_transformer_embeddings(df_test, CFG.ST_MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45b3032f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Generating KNN features for final models...\n"
     ]
    }
   ],
   "source": [
    "print(\"   Generating KNN features for final models...\")\n",
    "full_train_bge_embeds = combined_embeddings[:, bge_start_index:bge_end_index]\n",
    "test_bge_embeds = test_embeddings[:, bge_start_index:bge_end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596e4ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN features for the final LGBM training set\n",
    "final_train_knn_feats = generate_knn_features(full_train_bge_embeds, y, full_train_bge_embeds, CFG.KNN_N_NEIGHBORS)\n",
    "# KNN features for the test set (searching within the training data)\n",
    "test_knn_feats = generate_knn_features(full_train_bge_embeds, y, test_bge_embeds, CFG.KNN_N_NEIGHBORS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db41f556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(75000, 3)\n",
      "[[ 8.7405      4.745      13.03754318]\n",
      " [44.5675     17.745      63.02194702]\n",
      " [ 8.8295      6.73        5.21788868]\n",
      " [61.4225     26.1475     97.51395606]\n",
      " [24.7875     20.34       19.72900482]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_253/88841002.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_train_knn_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_train_knn_feats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfinal_train_knn_feats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "print(type(final_train_knn_feats))\n",
    "print(final_train_knn_feats.shape)\n",
    "print(final_train_knn_feats[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bdcb756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2/4: Re-training base models on 100% of the training data...\n",
      "   Training final XGBoost model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [12:09:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [12:09:59] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training final LightGBM model...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 686802\n",
      "[LightGBM] [Info] Number of data points in the train set: 75000, number of used features: 2694\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 2694 dense feature groups (192.83 MB) transferred to GPU in 0.179793 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 23.647654\n",
      "   Training final DenseNet model...\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2/4: Re-training base models on 100% of the training data...\")\n",
    "\n",
    "# --- Final Model 1: XGBoost ---\n",
    "print(\"   Training final XGBoost model...\")\n",
    "full_train_xgb = scipy.sparse.hstack((full_train_tfidf, full_train_engineered.values))\n",
    "X_test_xgb = scipy.sparse.hstack((X_test_tfidf, test_engineered.values))\n",
    "\n",
    "final_xgb_model = xgb.XGBRegressor(tree_method='gpu_hist', random_state=CFG.RANDOM_STATE)\n",
    "final_xgb_model.fit(full_train_xgb, y)\n",
    "test_preds_xgb = final_xgb_model.predict(X_test_xgb)\n",
    "\n",
    "# --- Final Model 2: LightGBM ---\n",
    "print(\"   Training final LightGBM model...\")\n",
    "full_train_lgbm = np.hstack([combined_embeddings, full_train_engineered.values, final_train_knn_feats])\n",
    "X_test_lgbm = np.hstack([test_embeddings, test_engineered.values, test_knn_feats])\n",
    "\n",
    "final_lgbm_model = lgb.LGBMRegressor(device='gpu', random_state=CFG.RANDOM_STATE)\n",
    "final_lgbm_model.fit(full_train_lgbm, y)\n",
    "test_preds_lgbm = final_lgbm_model.predict(X_test_lgbm)\n",
    "\n",
    "# --- Final Model 3: DenseNet (1D CNN) ---\n",
    "print(\"   Training final DenseNet model...\")\n",
    "final_tokenizer = Tokenizer(num_words=CFG.CNN_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "final_tokenizer.fit_on_texts(df['catalog_content'])\n",
    "\n",
    "full_train_seq = pad_sequences(final_tokenizer.texts_to_sequences(df['catalog_content']), maxlen=CFG.CNN_MAX_LEN)\n",
    "X_test_seq = pad_sequences(final_tokenizer.texts_to_sequences(X_test_text), maxlen=CFG.CNN_MAX_LEN)\n",
    "\n",
    "# Define the model architecture again (for a clean run)\n",
    "input_text = Input(shape=(CFG.CNN_MAX_LEN,))\n",
    "embedding = Embedding(CFG.CNN_VOCAB_SIZE, CFG.CNN_EMBEDDING_DIM)(input_text)\n",
    "conv1 = Conv1D(filters=64, kernel_size=3, activation='relu')(embedding)\n",
    "pool1 = GlobalMaxPooling1D()(conv1)\n",
    "dense1 = Dense(64, activation='relu')(pool1)\n",
    "dropout1 = Dropout(0.5)(dense1)\n",
    "output = Dense(1)(dropout1)\n",
    "final_cnn_model = Model(inputs=input_text, outputs=output)\n",
    "final_cnn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train on the full data. No validation or early stopping needed here.\n",
    "final_cnn_model.fit(full_train_seq, y, epochs=15, batch_size=128, verbose=0)\n",
    "test_preds_densenet = final_cnn_model.predict(X_test_seq, batch_size=512).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62ed5ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3/4: Making final predictions with the meta-model...\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Make Final Predictions with the Meta-Model ---\n",
    "print(\"\\n3/4: Making final predictions with the meta-model...\")\n",
    "# Stack the test predictions from the base models\n",
    "X_test_meta = np.column_stack((test_preds_xgb, test_preds_lgbm, test_preds_densenet))\n",
    "\n",
    "# CRUCIALLY, we DO NOT re-train the meta-model. We use the one trained on our OOF predictions.\n",
    "final_predictions = meta_model.predict(X_test_meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ec5a40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4/4: Creating submission.csv...\n",
      "\n",
      "Submission file created successfully!\n",
      "Top 5 rows of submission.csv:\n",
      "   sample_id      price\n",
      "0     100179  20.190771\n",
      "1     245611  15.532209\n",
      "2     146263  29.531922\n",
      "3      95658  10.189304\n",
      "4      36806  51.957387\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 5. Create the Submission File ---\n",
    "print(\"\\n4/4: Creating submission.csv...\")\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_id': test_ids,\n",
    "    'price': final_predictions\n",
    "})\n",
    "\n",
    "# Apply the safety net to ensure no negative prices\n",
    "submission_df['price'] = submission_df['price'].clip(0)\n",
    "\n",
    "submission_df.to_csv('submission1.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file created successfully!\")\n",
    "print(\"Top 5 rows of submission.csv:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69d6b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "982aee9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving combined embeddings to a file...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_78/180641957.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving combined embeddings to a file...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'combined_embeddings.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Embeddings saved!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Saving combined embeddings to a file...\")\n",
    "np.save('combined_embeddings.npy', combined_embeddings)\n",
    "print(\"Embeddings saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1476062f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
